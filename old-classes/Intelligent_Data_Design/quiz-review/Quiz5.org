
* 1.
#+begin_quote
Let \(x_1,x_2,x_3 \) represent 3 features. Which of the following are NOT linear combinations of these features?
#+end_quote
** Answer
+ [ ] \(0.4x_1 + 0.3x_2 + 0.6x_3\)
+ [x] \(4x_1^2 + 3x_2^2 + x_3^2\)
+ [ ] \(4^2 x_1 + 3^2 x_2 + 6^2 x_3\)
+ [ ] \(4x_1 + 3x_2 + 6x_3\)

* Question 2
#+begin_quote
Which one of the following statements about PCA is false?
#+end_quote
** Answer
+ [ ] PCA projects the attributes into a space where covariance matrix is diagonal
+ [ ] The first Principal Component points in the direction of maximum variance
+ [x] PCA is a non-linear dimensionality reduction technique
+ [ ] PCA is useful for exploratory data analysis

* Question 3
#+begin_quote
Which one of the following statements about PCA is false?
#+end_quote
** Answer
+ [x] PCA works well for circular data
+ [ ] The first PC points to maximum variance
+ [ ] PCA computes eigen-value eigen-vector decomposition of the covariance matrix
+ [ ] PCA works well for ellipsoidal data

* Question 4
#+begin_quote
The magnitude of vector x projected onto a unit vector u is
#+end_quote
** Answer
+ [ ] \(x \times u\)
+ [ ] \((x - \mu_x) \cdot (u - \mu_u)\)
+ [x] \(x\cdot u\)
+ [ ] \(||x||||u||\)

* Question 5
#+begin_quote
Feature selection is:
#+end_quote
** Answer
+ [x] selecting a subset of attributes
+ [ ] selecting principal components with maximum variance
+ [ ] combining many features into one
+ [ ] selecting principal components that are not orthogonal to each other
