{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name: Mohini Khedekar\n",
        "Date: April 13th 2025\n",
        "Data Science Project 3\n",
        "Link: https://colab.research.google.com/drive/1Z_fxY_cXVsYBr-YoOiDny0rcDw66xRFP?usp=sharing"
      ],
      "metadata": {
        "id": "DxBoHCDvm-RC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the template below to train and assess NN-based classification models for the prediction of breast cancer subtypes, following the overall strategy and R template used for Project 1 while replacing logistic regression with NN models."
      ],
      "metadata": {
        "id": "JM6pm2GlJnrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1a: Copy project1 files to your Google Drive/data_science directory and use those files to train and assess NN classifiers"
      ],
      "metadata": {
        "id": "A8uJ0h6B5FaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!ls /content/drive/MyDrive/data_science/\n",
        "\n",
        "# Copy the file to be used for the project from your data_science directory\n",
        "cancer_dataset_name = \"TCGA_breast_cancer_LumA_vs_Basal_PAM50.tsv\"\n",
        "!cp /content/drive/MyDrive/data_science/$cancer_dataset_name .\n",
        "!head -10 $cancer_dataset_name\n",
        "\n",
        "# Read the TSV data into a pandas DataFrame\n",
        "data = pd.read_csv(cancer_dataset_name, sep='\\t')\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "vHMgEyFZ7Ca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "043f9602-e8d7-470e-da0e-ede1b6c9b6ce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "TCGA_breast_cancer_ERpositive_vs_ERnegative_PAM50.tsv\n",
            "TCGA_breast_cancer_ERstatus_allGenes.txt\n",
            "TCGA_breast_cancer_LumA_vs_Basal_PAM50.tsv\n",
            "sample_id\tTCGA-A7-A13E-01A-11R-A12P-07\tTCGA-A8-A08H-01A-21R-A00Z-07\tTCGA-A8-A07U-01A-11R-A034-07\tTCGA-BH-A0E0-01A-11R-A056-07\tTCGA-A2-A04T-01A-21R-A034-07\tTCGA-BH-A18K-01A-11R-A12D-07\tTCGA-AQ-A04J-01A-02R-A034-07\tTCGA-A1-A0SK-01A-12R-A084-07\tTCGA-A7-A0DA-01A-31R-A115-07\tTCGA-AR-A1AI-01A-11R-A12P-07\tTCGA-E2-A14X-01A-11R-A115-07\tTCGA-A2-A0D0-01A-11R-A00Z-07\tTCGA-AN-A0FX-01A-11R-A034-07\tTCGA-E2-A14Y-01A-21R-A12D-07\tTCGA-AR-A0TU-01A-31R-A109-07\tTCGA-D8-A143-01A-11R-A115-07\tTCGA-AR-A0U4-01A-11R-A109-07\tTCGA-AR-A0U0-01A-11R-A109-07\tTCGA-A2-A0SX-01A-12R-A084-07\tTCGA-B6-A0I6-01A-11R-A034-07\tTCGA-E2-A159-01A-11R-A115-07\tTCGA-BH-A0B9-01A-11R-A056-07\tTCGA-E2-A150-01A-11R-A12D-07\tTCGA-A8-A07R-01A-21R-A034-07\tTCGA-AO-A0JL-01A-11R-A056-07\tTCGA-AN-A0G0-01A-11R-A034-07\tTCGA-BH-A0BL-01A-11R-A115-07\tTCGA-A2-A0T2-01A-11R-A084-07\tTCGA-BH-A0E6-01A-11R-A034-07\tTCGA-BH-A0RX-01A-21R-A084-07\tTCGA-B6-A0IQ-01A-11R-A034-07\tTCGA-AR-A1AJ-01A-21R-A12P-07\tTCGA-B6-A0RT-01A-21R-A084-07\tTCGA-BH-A0HN-01A-11R-A10U-07\tTCGA-E2-A14N-01A-31R-A137-07\tTCGA-BH-A0BW-01A-11R-A115-07\tTCGA-AN-A0AL-01A-11R-A00Z-07\tTCGA-E2-A158-01A-11R-A12D-07\tTCGA-A8-A08R-01A-11R-A034-07\tTCGA-A7-A0CE-01A-11R-A00Z-07\tTCGA-D8-A142-01A-11R-A115-07\tTCGA-AO-A128-01A-11R-A10J-07\tTCGA-A7-A13D-01A-13R-A12P-07\tTCGA-AO-A129-01A-21R-A10J-07\tTCGA-E2-A14R-01A-11R-A115-07\tTCGA-AN-A0FJ-01A-11R-A00Z-07\tTCGA-A2-A0YE-01A-11R-A109-07\tTCGA-AR-A1AH-01A-11R-A12D-07\tTCGA-AN-A0AT-01A-11R-A034-07\tTCGA-AO-A124-01A-11R-A10J-07\tTCGA-BH-A18G-01A-11R-A12D-07\tTCGA-B6-A0WX-01A-11R-A109-07\tTCGA-D8-A147-01A-11R-A115-07\tTCGA-BH-A0DL-01A-11R-A115-07\tTCGA-AO-A0J4-01A-11R-A034-07\tTCGA-BH-A18Q-01A-12R-A12D-07\tTCGA-AR-A1AQ-01A-11R-A12P-07\tTCGA-BH-A0B3-01A-11R-A056-07\tTCGA-A2-A0YJ-01A-11R-A109-07\tTCGA-C8-A12V-01A-11R-A115-07\tTCGA-A2-A0T0-01A-22R-A084-07\tTCGA-A2-A04U-01A-11R-A115-07\tTCGA-AR-A1AR-01A-31R-A137-07\tTCGA-B6-A0I2-01A-11R-A034-07\tTCGA-C8-A12K-01A-21R-A115-07\tTCGA-AN-A0XU-01A-11R-A109-07\tTCGA-E2-A1B5-01A-21R-A12P-07\tTCGA-BH-A0BG-01A-11R-A115-07\tTCGA-B6-A0X1-01A-11R-A109-07\tTCGA-C8-A134-01A-11R-A115-07\tTCGA-A8-A07O-01A-11R-A00Z-07\tTCGA-C8-A131-01A-11R-A115-07\tTCGA-BH-A0AV-01A-31R-A115-07\tTCGA-BH-A1F0-01A-11R-A137-07\tTCGA-A2-A04P-01A-31R-A034-07\tTCGA-A8-A07C-01A-11R-A034-07\tTCGA-AR-A0TP-01A-11R-A084-07\tTCGA-A1-A0SO-01A-22R-A084-07\tTCGA-AO-A0J6-01A-11R-A034-07\tTCGA-AO-A12F-01A-11R-A115-07\tTCGA-E2-A1AZ-01A-11R-A12P-07\tTCGA-AN-A0AR-01A-11R-A00Z-07\tTCGA-A2-A0YM-01A-11R-A109-07\tTCGA-B6-A0RE-01A-11R-A056-07\tTCGA-B6-A0RU-01A-11R-A084-07\tTCGA-BH-A18V-01A-11R-A12D-07\tTCGA-BH-A0WA-01A-11R-A109-07\tTCGA-A2-A04Q-01A-21R-A034-07\tTCGA-AN-A0FL-01A-11R-A034-07\tTCGA-B6-A0IJ-01A-11R-A034-07\tTCGA-A2-A0D2-01A-21R-A034-07\tTCGA-AR-A0TS-01A-11R-A115-07\tTCGA-A2-A0ST-01A-12R-A084-07\tTCGA-BH-A0HL-01A-11R-A10U-07\tTCGA-AN-A04D-01A-21R-A034-07\tTCGA-A2-A0CM-01A-31R-A034-07\tTCGA-AR-A1AY-01A-21R-A12P-07\tTCGA-E2-A154-01A-11R-A115-07\tTCGA-B6-A0IO-01A-11R-A034-07\tTCGA-BH-A0BT-01A-11R-A12P-07\tTCGA-AO-A0J9-01A-11R-A034-07\tTCGA-A8-A08T-01A-21R-A00Z-07\tTCGA-E2-A106-01A-11R-A10J-07\tTCGA-A2-A04V-01A-21R-A034-07\tTCGA-AO-A0JJ-01A-11R-A056-07\tTCGA-AR-A1AS-01A-11R-A12P-07\tTCGA-A8-A09B-01A-11R-A00Z-07\tTCGA-B6-A0IN-01A-11R-A034-07\tTCGA-A2-A04N-01A-11R-A115-07\tTCGA-BH-A0W5-01A-11R-A109-07\tTCGA-A8-A0A4-01A-11R-A00Z-07\tTCGA-AR-A1AN-01A-11R-A12P-07\tTCGA-A2-A04Y-01A-21R-A034-07\tTCGA-A2-A0EU-01A-22R-A056-07\tTCGA-BH-A0H5-01A-21R-A115-07\tTCGA-A2-A0ES-01A-11R-A115-07\tTCGA-A1-A0SJ-01A-11R-A084-07\tTCGA-AO-A0J8-01A-21R-A034-07\tTCGA-C8-A132-01A-31R-A115-07\tTCGA-A8-A086-01A-11R-A00Z-07\tTCGA-AN-A0XP-01A-11R-A109-07\tTCGA-A2-A0CU-01A-12R-A034-07\tTCGA-BH-A18T-01A-11R-A12D-07\tTCGA-A8-A07J-01A-11R-A00Z-07\tTCGA-B6-A0WY-01A-11R-A109-07\tTCGA-AN-A04A-01A-21R-A034-07\tTCGA-A2-A0YI-01A-31R-A10J-07\tTCGA-A8-A09V-01A-11R-A034-07\tTCGA-B6-A0X4-01A-11R-A109-07\tTCGA-BH-A0B0-01A-21R-A115-07\tTCGA-AN-A0XO-01A-11R-A109-07\tTCGA-BH-A0AZ-01A-21R-A12P-07\tTCGA-A7-A0CD-01A-11R-A00Z-07\tTCGA-B6-A0RN-01A-12R-A084-07\tTCGA-BH-A0B8-01A-21R-A056-07\tTCGA-BH-A1ES-01A-11R-A137-07\tTCGA-A8-A07F-01A-11R-A00Z-07\tTCGA-BH-A0GY-01A-11R-A056-07\tTCGA-BH-A18I-01A-11R-A12D-07\tTCGA-AR-A1AP-01A-11R-A12P-07\tTCGA-A2-A0YD-01A-11R-A109-07\tTCGA-E2-A15J-01A-11R-A12P-07\tTCGA-A2-A0CS-01A-11R-A115-07\tTCGA-BH-A0DG-01A-21R-A12P-07\tTCGA-B6-A0I8-01A-11R-A034-07\tTCGA-BH-A0HP-01A-12R-A084-07\tTCGA-BH-A0BV-01A-11R-A00Z-07\tTCGA-BH-A0BA-01A-11R-A056-07\tTCGA-A7-A0DB-01A-11R-A00Z-07\tTCGA-BH-A1EU-01A-11R-A137-07\tTCGA-BH-A0H9-01A-11R-A056-07\tTCGA-AN-A0XT-01A-11R-A109-07\tTCGA-A8-A0AD-01A-11R-A056-07\tTCGA-A2-A0YL-01A-21R-A109-07\tTCGA-A8-A0A1-01A-11R-A00Z-07\tTCGA-A7-A0D9-01A-31R-A056-07\tTCGA-BH-A0DX-01A-11R-A115-07\tTCGA-A8-A099-01A-11R-A00Z-07\tTCGA-E2-A10F-01A-11R-A10J-07\tTCGA-A2-A0T6-01A-11R-A084-07\tTCGA-E2-A1B4-01A-11R-A12P-07\tTCGA-BH-A0DI-01A-21R-A12P-07\tTCGA-A8-A0A6-01A-12R-A056-07\tTCGA-D8-A145-01A-11R-A115-07\tTCGA-AN-A0XN-01A-21R-A109-07\tTCGA-A2-A0CZ-01A-11R-A034-07\tTCGA-AN-A0FT-01A-11R-A034-07\tTCGA-BH-A0E7-01A-11R-A034-07\tTCGA-A1-A0SD-01A-11R-A115-07\tTCGA-BH-A0DT-01A-21R-A12D-07\tTCGA-BH-A18H-01A-11R-A12D-07\tTCGA-AR-A1AL-01A-21R-A12P-07\tTCGA-E2-A1BC-01A-11R-A12P-07\tTCGA-A8-A06U-01A-11R-A00Z-07\tTCGA-BH-A18M-01A-11R-A12D-07\tTCGA-C8-A12N-01A-11R-A115-07\tTCGA-A8-A090-01A-11R-A00Z-07\tTCGA-BH-A0W7-01A-11R-A115-07\tTCGA-A2-A0SU-01A-11R-A084-07\tTCGA-B6-A0RI-01A-11R-A056-07\tTCGA-B6-A0RM-01A-11R-A084-07\tTCGA-B6-A0RG-01A-11R-A056-07\tTCGA-E2-A15I-01A-21R-A137-07\tTCGA-AO-A12C-01A-11R-A10J-07\tTCGA-A8-A09A-01A-11R-A00Z-07\tTCGA-E2-A153-01A-12R-A12D-07\tTCGA-E2-A1B1-01A-21R-A12P-07\tTCGA-BH-A0HA-01A-11R-A12P-07\tTCGA-BH-A0E2-01A-11R-A056-07\tTCGA-A2-A0CP-01A-11R-A034-07\tTCGA-C8-A12O-01A-11R-A115-07\tTCGA-AR-A1AU-01A-11R-A12P-07\tTCGA-A1-A0SH-01A-11R-A084-07\tTCGA-A7-A0CG-01A-12R-A056-07\tTCGA-AN-A0FD-01A-11R-A034-07\tTCGA-AO-A12H-01A-11R-A115-07\tTCGA-A2-A0YC-01A-11R-A109-07\tTCGA-BH-A0DS-01A-11R-A056-07\tTCGA-AR-A1AW-01A-21R-A12P-07\tTCGA-A2-A0CQ-01A-21R-A034-07\tTCGA-BH-A0HI-01A-11R-A084-07\tTCGA-A1-A0SE-01A-11R-A084-07\tTCGA-BH-A0EA-01A-11R-A115-07\tTCGA-BH-A0BO-01A-23R-A12D-07\tTCGA-E2-A105-01A-11R-A10J-07\tTCGA-E2-A10E-01A-21R-A10J-07\tTCGA-A2-A0CV-01A-31R-A115-07\tTCGA-E2-A15E-01A-11R-A12D-07\tTCGA-BH-A18N-01A-11R-A12D-07\tTCGA-AR-A1AK-01A-21R-A12P-07\tTCGA-A8-A093-01A-11R-A00Z-07\tTCGA-BH-A0DO-01B-11R-A12D-07\tTCGA-E2-A15F-01A-11R-A115-07\tTCGA-BH-A0BR-01A-21R-A115-07\tTCGA-AO-A03M-01B-11R-A10J-07\tTCGA-B6-A0IA-01A-11R-A034-07\tTCGA-BH-A0HQ-01A-11R-A034-07\tTCGA-B6-A0I5-01A-11R-A034-07\tTCGA-BH-A0GZ-01A-11R-A056-07\tTCGA-AO-A12A-01A-21R-A115-07\tTCGA-AN-A046-01A-21R-A034-07\tTCGA-B6-A0RV-01A-11R-A084-07\tTCGA-B6-A0RQ-01A-11R-A115-07\tTCGA-D8-A141-01A-11R-A115-07\tTCGA-BH-A0BS-01A-11R-A12P-07\tTCGA-E2-A14Q-01A-11R-A12D-07\tTCGA-A2-A0YF-01A-21R-A109-07\tTCGA-A7-A0CH-01A-21R-A00Z-07\tTCGA-E2-A15P-01A-11R-A115-07\tTCGA-A8-A08O-01A-21R-A056-07\tTCGA-BH-A0HX-01A-21R-A056-07\tTCGA-BH-A1EO-01A-11R-A137-07\tTCGA-C8-A12Y-01A-11R-A12D-07\tTCGA-B6-A0IH-01A-11R-A115-07\tTCGA-B6-A0WT-01A-11R-A109-07\tTCGA-BH-A0DE-01A-11R-A115-07\tTCGA-E2-A156-01A-11R-A12D-07\tTCGA-BH-A0B4-01A-11R-A00Z-07\tTCGA-BH-A0DP-01A-21R-A056-07\tTCGA-A8-A091-01A-11R-A00Z-07\tTCGA-A2-A0EN-01A-13R-A084-07\tTCGA-BH-A0HB-01A-11R-A056-07\tTCGA-AN-A0XS-01A-22R-A109-07\tTCGA-E2-A15O-01A-11R-A115-07\tTCGA-AR-A0TR-01A-11R-A084-07\tTCGA-BH-A0B1-01A-12R-A056-07\tTCGA-A8-A08Z-01A-21R-A00Z-07\tTCGA-BH-A0DH-01A-11R-A084-07\tTCGA-B6-A0RO-01A-22R-A084-07\tTCGA-B6-A0X0-01A-21R-A115-07\tTCGA-A8-A06T-01A-11R-A00Z-07\tTCGA-AR-A0TW-01A-11R-A084-07\tTCGA-BH-A0BM-01A-11R-A056-07\tTCGA-A2-A0SY-01A-31R-A084-07\tTCGA-A8-A07P-01A-11R-A00Z-07\tTCGA-AQ-A04L-01B-21R-A10J-07\tTCGA-A8-A0A2-01A-11R-A034-07\tTCGA-E2-A15R-01A-11R-A115-07\tTCGA-A2-A0T5-01A-21R-A084-07\tTCGA-BH-A0BP-01A-11R-A115-07\tTCGA-AO-A125-01A-11R-A10J-07\tTCGA-AO-A0JG-01A-31R-A084-07\tTCGA-A8-A08C-01A-11R-A00Z-07\tTCGA-BH-A0HO-01A-11R-A034-07\tTCGA-C8-A133-01A-32R-A12D-07\tTCGA-E2-A14Z-01A-11R-A115-07\tTCGA-A8-A06Y-01A-21R-A00Z-07\tTCGA-E2-A15C-01A-31R-A12D-07\tTCGA-AN-A0FS-01A-11R-A034-07\tTCGA-E2-A10B-01A-11R-A10J-07\tTCGA-A2-A0D3-01A-11R-A115-07\tTCGA-A2-A0T7-01A-21R-A084-07\tTCGA-C8-A1HI-01A-11R-A137-07\tTCGA-A8-A08A-01A-11R-A32Y-07\tTCGA-AN-A0FW-01A-11R-A034-07\tTCGA-BH-A0BJ-01A-11R-A056-07\tTCGA-A8-A083-01A-21R-A00Z-07\tTCGA-A2-A0EW-01A-21R-A115-07\tTCGA-AO-A126-01A-11R-A10J-07\tTCGA-BH-A0BC-01A-22R-A084-07\tTCGA-AN-A0FZ-01A-11R-A034-07\tTCGA-AO-A03V-01A-11R-A115-07\tTCGA-AO-A12G-01A-11R-A10J-07\tTCGA-AO-A0JA-01A-11R-A056-07\tTCGA-BH-A0H3-01A-11R-A12P-07\tTCGA-A2-A0EV-01A-11R-A034-07\tTCGA-BH-A0C1-01B-11R-A12D-07\tTCGA-AR-A1AX-01A-11R-A12P-07\tTCGA-A2-A0EX-01A-21R-A034-07\tTCGA-BH-A1ET-01A-11R-A137-07\tTCGA-AO-A0JF-01A-11R-A056-07\tTCGA-B6-A0WS-01A-11R-A115-07\tTCGA-E2-A15G-01A-11R-A12D-07\tTCGA-BH-A0DK-01A-21R-A056-07\tTCGA-BH-A0H6-01A-21R-A056-07\tTCGA-AN-A0FN-01A-11R-A034-07\tTCGA-BH-A0DQ-01A-11R-A084-07\tTCGA-BH-A18S-01A-11R-A12D-07\tTCGA-B6-A0WZ-01A-11R-A109-07\tTCGA-A8-A07G-01A-11R-A034-07\tTCGA-A2-A0EM-01A-11R-A034-07\tTCGA-BH-A0H7-01A-13R-A056-07\tTCGA-E2-A15D-01A-11R-A115-07\tTCGA-E2-A1BD-01A-11R-A12P-07\tTCGA-A8-A06P-01A-11R-A00Z-07\tTCGA-E2-A15H-01A-11R-A12D-07\tTCGA-BH-A0EI-01A-11R-A115-07\tTCGA-A8-A07E-01A-11R-A034-07\tTCGA-E2-A14T-01A-11R-A115-07\tTCGA-BH-A0E9-01B-11R-A115-07\tTCGA-BH-A0EB-01A-11R-A034-07\tTCGA-BH-A0E1-01A-11R-A056-07\tTCGA-BH-A0W4-01A-11R-A109-07\tTCGA-E2-A1B6-01A-31R-A12P-07\tTCGA-AN-A0XL-01A-11R-A10J-07\tTCGA-BH-A0BQ-01A-21R-A115-07\tTCGA-AO-A12E-01A-11R-A10J-07\tTCGA-A2-A0EO-01A-11R-A034-07\tTCGA-B6-A0IP-01A-11R-A034-07\tTCGA-A8-A09T-01A-11R-A00Z-07\tTCGA-AN-A03X-01A-21R-A00Z-07\tTCGA-BH-A0HF-01A-11R-A056-07\tTCGA-B6-A0IG-01A-11R-A034-07\tTCGA-AN-A0XV-01A-11R-A109-07\tTCGA-B6-A0RP-01A-21R-A084-07\tTCGA-B6-A0X7-01A-11R-A10J-07\tTCGA-BH-A0HK-01A-11R-A056-07\tTCGA-A2-A0ET-01A-31R-A034-07\n",
            "class\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tBasal-like\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\tLuminal A\n",
            "NAT1\t3.59727\t7.2052\t5.12902\t4.82895\t4.79575\t10.1133\t6.60699\t2.59515\t6.09316\t6.76681\t4.91686\t5.57108\t4.86147\t3.66065\t4.68538\t6.09457\t8.47517\t4.99638\t5.63842\t4.43973\t7.69908\t7.00709\t4.83386\t5.62408\t7.16429\t4.42743\t7.77036\t5.0871\t7.32924\t7.83876\t5.69183\t6.39497\t6.73965\t13.7348\t5.67337\t6.44047\t7.06013\t5.46545\t5.96591\t6.38163\t6.35772\t6.77184\t5.98998\t5.8325\t6.08639\t4.37906\t6.20706\t7.89955\t4.85608\t5.36047\t6.27146\t7.25089\t6.03127\t6.06803\t4.88515\t6.32575\t5.21556\t6.80177\t4.0761\t7.41367\t6.09102\t6.32141\t6.60206\t5.77252\t6.75532\t6.85914\t10.2338\t6.04558\t5.22566\t5.53356\t7.27061\t7.1251\t4.53372\t6.46469\t6.28102\t6.17546\t2.685\t5.91833\t6.02422\t5.0438\t7.01438\t4.4301\t4.63542\t6.53051\t6.24542\t6.07931\t5.08065\t5.36135\t6.29248\t5.18255\t4.38952\t5.97467\t7.75884\t8.04102\t3.80293\t5.82894\t3.97236\t13.2257\t10.2645\t7.12344\t11.2243\t11.9719\t11.1907\t11.2731\t10.7708\t9.98784\t13.3846\t11.1808\t10.4642\t7.16313\t9.58068\t11.4563\t12.3195\t7.39469\t12.232\t9.36155\t8.17251\t13.3288\t9.96778\t10.6225\t14.05\t7.69478\t5.21777\t11.8753\t10.1872\t11.3405\t12.5139\t12.7415\t12.3258\t11.4195\t12.7234\t11.3836\t10.9929\t13.0191\t11.0446\t6.59845\t13.2143\t10.6116\t11.2163\t10.3499\t12.5892\t12.2339\t13.9648\t10.355\t12.1705\t9.10604\t9.23159\t10.6222\t12.3081\t13.7436\t12.0592\t8.14757\t9.62352\t10.0096\t13.3728\t7.7067\t13.1339\t14.2701\t11.3547\t11.046\t13.5953\t9.04666\t11.3618\t10.3936\t6.94884\t7.20154\t8.25415\t10.236\t10.8403\t13.9561\t11.551\t11.4547\t11.277\t10.3974\t10.6285\t12.6909\t9.4361\t11.2\t7.49937\t10.5503\t8.22865\t11.6563\t13.2085\t11.0728\t8.03834\t11.3483\t7.5227\t12.1469\t9.13097\t10.5556\t7.0309\t7.33414\t8.69906\t13.4134\t9.51003\t9.93583\t12.8134\t12.9732\t10.1212\t13.8364\t14.0064\t11.9715\t12.4289\t11.9787\t11.748\t13.538\t13.5924\t10.3326\t9.57718\t9.09661\t13.7407\t11.6247\t8.88942\t9.30845\t9.98655\t10.3865\t12.8297\t13.9295\t12.6773\t13.4367\t13.0514\t11.8471\t7.40968\t12.7977\t13.9451\t12.8261\t12.4406\t13.9666\t13.3347\t9.93521\t10.1715\t10.8161\t10.4427\t11.587\t11.646\t12.0076\t12.8706\t11.7182\t11.8527\t7.1423\t11.8425\t10.8769\t11.9492\t11.9277\t13.0529\t10.8733\t8.75207\t12.09\t13.0993\t11.8249\t11.3135\t9.4571\t12.7466\t12.4834\t11.7538\t7.83012\t13.3809\t10.4864\t12.9134\t12.5466\t13.8958\t7.91779\t13.7459\t13.206\t7.13259\t10.7848\t12.2354\t12.2586\t10.5753\t12.7699\t14.5245\t8.90847\t11.7858\t12.9426\t12.1375\t13.7989\t11.8082\t11.9531\t7.83013\t12.2824\t11.4238\t11.1477\t11.7224\t13.6311\t12.8494\t10.6877\t11.9272\t8.84186\t9.71854\t13.9985\t13.4478\t9.64554\t13.7288\t10.5706\t8.16842\t12.5354\t11.8123\t9.74094\t13.0772\t11.942\t13.4984\t11.0957\t13.8852\t14.0821\t8.44702\t11.4563\t6.84478\t11.444\t8.53487\t11.8001\t13.5225\t13.7148\t11.0163\t6.7291\t11.4509\t12.444\t12.747\t12.9168\t12.9465\t10.1884\t13.1863\t12.2478\t11.2064\t10.4554\t13.6232\t10.5333\t11.9222\t13.7303\n",
            "BIRC5\t8.38208\t8.53294\t9.01439\t8.74525\t10.1728\t8.38562\t9.3852\t9.40988\t9.26187\t9.72879\t8.16211\t10.9978\t8.67507\t10.0907\t11.3241\t10.332\t9.38954\t8.84201\t7.9165\t8.95884\t8.9104\t10.2382\t8.46744\t11.0621\t9.53513\t9.57486\t9.03905\t8.24121\t10.3443\t10.8215\t9.96716\t9.19874\t9.2451\t6.6299\t10.4579\t9.73368\t10.2856\t8.54969\t10.267\t10.1316\t8.83027\t8.36718\t10.313\t9.82764\t9.78964\t9.48983\t9.87151\t10.8585\t10.5273\t9.35213\t10.3548\t8.29046\t9.84041\t8.29078\t9.28679\t9.19212\t10.1569\t10.9194\t9.40675\t9.62802\t10.286\t11.0082\t8.73541\t9.37694\t10.4089\t10.6167\t7.51679\t9.3892\t10.6533\t10.9601\t10.7391\t9.77814\t9.90162\t9.55388\t11.5713\t10.1778\t9.75817\t10.6315\t10.6511\t9.98161\t9.67617\t10.7734\t10.5091\t10.174\t9.92915\t9.98097\t9.78889\t9.69219\t9.76047\t10.4017\t10.8177\t9.78566\t8.58896\t8.20042\t11.2173\t9.95946\t9.53365\t7.95658\t8.26363\t7.73463\t7.84126\t7.37731\t7.36349\t8.7484\t7.81078\t8.08713\t6.73901\t9.2245\t7.13935\t6.09253\t8.02585\t7.40126\t8.8312\t8.28444\t6.88729\t6.50421\t8.37421\t7.29136\t6.92973\t8.72792\t6.61807\t8.40832\t9.62802\t6.07116\t7.21541\t7.4797\t5.48105\t6.9843\t6.02504\t6.51348\t7.93061\t7.78047\t8.14168\t6.04951\t7.9417\t9.26909\t7.40226\t8.69631\t7.26372\t8.0031\t6.79971\t7.69951\t7.16074\t9.53055\t7.75115\t7.82673\t7.55923\t7.99846\t7.33705\t5.71288\t8.82431\t6.62177\t7.16444\t8.2723\t7.33868\t6.42568\t8.6172\t6.24608\t8.08874\t5.12926\t5.79462\t7.36328\t6.33994\t6.53956\t8.048\t6.97608\t7.40872\t7.17536\t7.39422\t7.031\t7.55223\t6.33792\t4.01179\t8.75633\t7.57003\t6.63876\t8.76937\t8.16788\t9.18736\t9.69378\t7.71919\t9.0185\t3.59133\t6.62813\t8.09509\t6.62819\t9.0393\t8.00986\t9.94855\t5.20049\t8.658\t8.31956\t7.41558\t7.26125\t8.41251\t7.70834\t7.99052\t6.32178\t8.08714\t8.24509\t6.57879\t8.1064\t5.69476\t4.70906\t9.50191\t7.56515\t3.24682\t7.87283\t6.98695\t8.69035\t7.85144\t4.86154\t8.17312\t8.35555\t8.95224\t8.21016\t7.05779\t8.76599\t8.25658\t6.84966\t7.93863\t7.5809\t5.81399\t6.93785\t7.70023\t8.10222\t8.5025\t6.88568\t6.29678\t7.35009\t9.12791\t6.79254\t8.95625\t4.28812\t7.66088\t7.48151\t7.43363\t9.94164\t6.55084\t7.08883\t6.62694\t9.39371\t6.61299\t8.49625\t6.13851\t8.47584\t7.36373\t7.51803\t6.81264\t8.06569\t7.50115\t7.76938\t5.01001\t7.7707\t8.95199\t8.18176\t7.06898\t8.33897\t8.01306\t7.15765\t7.81563\t5.22526\t7.20393\t7.17981\t7.65245\t7.66995\t6.73798\t5.84659\t6.71826\t7.34611\t4.78305\t7.71719\t7.70953\t7.30624\t8.36303\t6.86146\t5.18309\t6.12528\t8.75171\t8.03976\t7.88603\t6.33873\t7.94806\t8.77425\t6.12114\t8.89655\t7.27841\t8.07975\t6.18672\t5.91736\t6.78328\t8.0329\t6.17231\t9.20778\t6.16001\t7.24061\t7.83664\t6.12939\t8.36232\t7.36831\t7.40718\t8.56995\t5.77537\t7.85632\t6.71604\t9.27684\t6.71596\t8.41472\t7.60897\t6.51982\t6.9568\t8.91842\t7.66371\t10.074\t7.49767\t7.05865\t8.39563\t7.02236\t7.82759\t7.31434\t8.34027\t8.30837\t8.07246\t7.66668\t6.15863\t6.69209\t8.26247\t7.38246\n",
            "BAG1\t11.0157\t10.3315\t10.5027\t10.5346\t9.39239\t10.2508\t10.0683\t10.4644\t10.608\t8.31543\t11.2729\t10.0022\t10.2173\t12.388\t9.5032\t8.38815\t9.0145\t10.1355\t10.0774\t9.6995\t9.68748\t11.167\t9.44083\t9.56979\t9.95235\t9.85484\t8.78969\t10.2741\t9.97119\t10.55\t10.5376\t9.48478\t9.91844\t11.474\t10.7459\t8.11743\t9.72547\t10.9524\t9.2942\t11.9547\t9.9472\t9.88042\t9.25799\t10.6045\t10.8145\t10.0336\t8.32026\t9.51525\t10.0747\t9.63106\t10.132\t9.78676\t9.40446\t9.35555\t9.36828\t9.78242\t10.1658\t9.82318\t9.93367\t9.55468\t9.85891\t10.7687\t10.1683\t12.5155\t9.8953\t9.8374\t10.8594\t10.0084\t9.99856\t10.0973\t10.5171\t9.95748\t10.1582\t11.2058\t9.93332\t9.8136\t9.6572\t8.49289\t9.83061\t10.4479\t10.0255\t11.5525\t9.67086\t10.8385\t10.3546\t10.7461\t9.35533\t10.6765\t11.0382\t10.7782\t9.57027\t9.08617\t10.3046\t11.5373\t10.0129\t10.912\t9.72386\t11.7333\t9.81151\t10.8897\t10.663\t10.8776\t11.0292\t12.0737\t11.0006\t10.211\t9.86952\t11.1715\t10.3127\t11.0355\t10.6066\t11.6558\t11.5609\t10.7533\t10.3262\t10.405\t9.50875\t9.94866\t9.81933\t10.5001\t10.4969\t10.5946\t9.24425\t9.68364\t10.3528\t10.3115\t10.5053\t10.5117\t12.9078\t11.472\t10.8497\t10.174\t12.0427\t10.4808\t11.851\t10.2476\t10.4164\t9.71898\t10.2874\t10.0076\t10.1488\t11.6095\t11.1409\t10.0818\t10.3235\t10.9111\t10.9382\t11.7053\t10.5376\t10.1965\t10.4046\t11.8845\t12.1616\t10.0138\t10.0511\t11.6578\t9.8253\t10.8929\t10.5424\t10.2724\t11.0027\t9.55581\t10.2626\t10.0119\t9.65371\t10.4164\t11.4155\t11.8926\t12.8259\t11.7667\t10.7666\t10.0948\t10.3362\t9.23394\t10.3798\t10.3053\t9.65377\t10.2153\t11.5912\t10.9555\t11.2067\t9.89668\t9.87789\t10.6547\t10.7295\t10.7768\t10.8194\t11.2384\t11.9474\t10.8854\t9.65114\t11.6404\t10.6872\t11.4418\t9.87071\t12.8239\t10.6495\t10.4335\t9.33152\t11.1814\t10.9749\t10.8907\t11.0728\t10.6333\t10.3996\t10.4012\t11.7694\t9.85682\t10.3459\t10.1229\t10.5718\t9.94246\t10.0467\t9.83119\t9.84914\t12.7738\t10.1673\t12.1431\t11.3855\t11.8907\t11.473\t10.9192\t10.5491\t10.2196\t10.3792\t10.8529\t13.2216\t10.5734\t11.5427\t10.5238\t10.3986\t10.5696\t10.9321\t10.1631\t11.0455\t11.4868\t11.2032\t10.8982\t10.1044\t11.9616\t10.4893\t10.6759\t9.76773\t10.7867\t10.4199\t9.94268\t9.9939\t11.803\t10.7947\t11.1296\t11.1051\t10.2266\t11.2803\t10.7254\t11.9722\t9.77276\t10.6044\t10.8818\t11.8111\t9.98573\t10.7783\t11.2328\t11.6591\t11.5702\t14.2353\t9.5863\t10.5188\t10.9498\t10.5532\t10.6911\t11.9319\t9.94331\t11.4221\t11.4837\t10.8939\t10.705\t11.3951\t10.0296\t11.0745\t10.6468\t10.5272\t9.96194\t10.4538\t9.46758\t11.3075\t10.2164\t10.0734\t9.91836\t10.6262\t10.2747\t10.3713\t11.4685\t10.3709\t11.8198\t11.0067\t10.352\t10.3756\t10.7957\t11.2788\t10.0502\t11.3033\t10.0227\t11.9224\t10.4191\t11.9577\t12.0782\t11.0578\t10.3925\t10.8722\t11.0187\t10.3855\t13.0571\t11.9525\t11.4251\t10.0343\t11.5304\t10.346\t11.0126\t10.7754\t10.4413\t9.73608\t10.5251\t11.5608\t10.7085\t10.281\t10.5999\t10.6069\t10.661\n",
            "BCL2\t10.3142\t9.25682\t8.77254\t8.37459\t8.62946\t9.31121\t9.41709\t12.361\t8.09499\t7.88591\t9.95322\t7.54178\t9.95615\t7.2815\t8.1433\t8.04805\t9.56499\t7.32459\t9.90044\t12.5524\t8.08607\t10.8399\t8.40824\t5.75812\t8.38708\t6.19187\t9.1898\t7.33538\t8.95922\t9.02219\t9.89682\t9.74105\t10.115\t11.1567\t8.38096\t8.88681\t7.92211\t10.5867\t8.53865\t7.90691\t8.03245\t8.2562\t6.98785\t8.44177\t7.82407\t11.6811\t7.84007\t8.58118\t7.80998\t6.50622\t7.37504\t9.0165\t8.25851\t7.71267\t8.21786\t12.1361\t8.98494\t7.37909\t5.30922\t9.82096\t6.8621\t9.25858\t9.59685\t9.3561\t11.1331\t7.3934\t11.1969\t10.2036\t9.03164\t8.32258\t9.119\t6.45322\t7.47816\t9.657\t7.87603\t9.81112\t10.9022\t6.06536\t8.04425\t7.82187\t9.5195\t10.3017\t8.80846\t7.68837\t11.0144\t8.94885\t7.24332\t8.94685\t5.55648\t9.86046\t8.94533\t9.92266\t10.7718\t11.8393\t9.25739\t7.87929\t10.3186\t10.0012\t10.6567\t7.61045\t10.507\t12.7299\t12.2987\t12.0068\t10.9334\t9.60386\t11.2534\t11.8642\t10.5119\t11.2055\t11.0712\t11.4075\t11.4019\t11.2026\t11.1147\t11.4611\t11.6347\t10.3937\t10.7193\t11.9496\t10.4493\t10.6063\t7.38977\t10.9099\t9.99147\t13.178\t11.5585\t12.0148\t12.24\t9.86102\t11.8852\t9.92305\t12.3696\t11.373\t12.9888\t11.2921\t10.9537\t8.63737\t11.4743\t9.48884\t11.1102\t12.0646\t11.4322\t10.1314\t8.4254\t10.8239\t11.2982\t10.3579\t10.6736\t12.2986\t10.2332\t13.1698\t11.8595\t11.2943\t10.9217\t12.1464\t10.1072\t11.5342\t10.9117\t10.5901\t11.6069\t10.6422\t10.6256\t11.6364\t10.6937\t10.2214\t8.68944\t11.9247\t10.9738\t12.3083\t10.2563\t11.2767\t10.996\t8.86958\t11.3644\t11.2759\t10.5612\t10.5008\t11.9623\t10.9998\t11.3624\t12.1973\t11.4785\t10.4869\t10.1601\t10.8963\t10.767\t12.1589\t11.5973\t10.2433\t8.45032\t12.1275\t10.2687\t10.2264\t12.2987\t13.4805\t11.8129\t9.96446\t10.656\t11.0083\t10.7469\t11.0156\t11.4872\t10.9431\t13.4264\t10.1897\t11.7702\t11.1502\t11.686\t10.614\t11.918\t11.1341\t11.554\t8.27218\t11.0641\t10.8914\t10.1837\t10.248\t11.2153\t12.0213\t11.9199\t10.6109\t10.9941\t10.2749\t11.3278\t11.1386\t12.2056\t11.6806\t11.8808\t12.8156\t10.6752\t10.4788\t10.7465\t11.352\t11.498\t11.8471\t12.1311\t9.52708\t11.2703\t12.4842\t9.6232\t12.0736\t11.9029\t11.6377\t12.1333\t10.2602\t10.1083\t9.60538\t10.3805\t11.6148\t13.1991\t11.8833\t11.0272\t11.6437\t12.9581\t8.30961\t11.8188\t10.8126\t11.4678\t11.4135\t10.6051\t12.1889\t12.9434\t11.9878\t11.8037\t9.57107\t10.7458\t12.0739\t11.3392\t11.3361\t11.6243\t9.59838\t10.0321\t10.9733\t12.2268\t11.3643\t12.0472\t10.7619\t12.0893\t11.2187\t10.425\t11.7963\t10.8375\t10.0729\t12.4764\t10.8621\t9.8542\t10.8125\t11.2075\t11.0738\t10.8125\t12.6661\t11.4369\t11.6283\t11.6242\t10.7708\t10.2691\t11.8795\t10.6647\t10.1313\t12.0525\t9.50099\t12.2528\t10.5561\t10.8453\t10.886\t10.8775\t10.2255\t11.644\t11.7211\t11.319\t11.5518\t12.0183\t10.2685\t10.5873\t11.1731\t9.80043\t10.9033\t11.7466\t11.3728\t9.38138\t11.2281\t10.3105\t12.0191\t10.781\t11.8249\t11.319\t11.9105\n",
            "BLVRA\t9.52925\t10.3032\t9.9443\t10.5833\t8.21056\t11.2039\t9.06456\t8.19551\t8.47337\t8.64231\t8.05863\t9.61462\t9.34528\t9.394\t8.29192\t9.7292\t8.53251\t9.39869\t9.16141\t8.70981\t10.3167\t9.40157\t8.3593\t8.98414\t9.92177\t8.72075\t9.53122\t9.41317\t9.72746\t10.327\t8.74188\t10.0613\t9.57982\t9.98406\t9.1057\t9.80496\t9.05746\t7.98636\t8.42893\t8.48783\t8.29652\t10.9278\t9.24958\t9.20975\t9.58958\t9.26736\t8.59712\t8.62718\t9.91907\t9.33997\t9.67904\t9.75193\t9.01277\t9.85287\t8.89381\t8.11249\t9.32297\t9.55594\t7.84068\t9.72675\t9.04132\t8.54302\t8.97577\t9.90378\t10.1416\t9.30287\t11.0131\t9.16944\t9.66916\t9.31841\t8.48886\t10.8404\t8.01058\t9.74569\t9.43868\t7.85374\t10.1843\t9.07125\t9.77135\t7.66382\t9.27765\t9.65782\t7.8809\t9.36213\t8.88544\t9.15161\t8.18529\t10.8453\t8.35974\t9.75911\t9.02221\t8.52508\t10.5572\t9.49786\t9.26753\t9.2032\t8.12099\t10.6766\t9.94826\t11.2953\t11.3177\t9.28377\t10.5077\t9.31602\t11.0092\t10.9897\t10.6313\t11.0197\t9.89522\t10.4907\t10.7755\t10.3594\t10.3433\t9.88729\t10.4057\t10.2313\t11.67\t10.1271\t10.6617\t10.2832\t8.79261\t10.047\t8.84583\t10.4634\t10.0401\t10.6039\t10.8949\t10.4384\t9.76479\t9.94503\t9.85313\t11.2557\t10.7008\t9.34414\t11.3289\t9.96604\t11.1752\t10.3577\t10.5007\t10.341\t10.1592\t9.1429\t9.89634\t10.8927\t10.2169\t10.4103\t11.1192\t10.2125\t9.96957\t10.0654\t10.7321\t10.873\t7.65729\t10.2323\t10.1117\t10.7173\t10.4584\t9.94908\t10.0334\t9.50304\t10.7053\t9.98159\t10.4924\t10.1073\t9.82908\t10.2527\t11.7364\t10.3306\t10.1456\t10.1999\t10.1153\t9.55232\t10.3444\t10.1482\t10.2432\t10.3951\t10.8253\t10.9049\t7.92949\t10.4337\t7.19456\t10.6167\t9.65156\t10.0774\t10.3164\t9.91831\t10.3442\t11.7741\t9.45198\t10.3293\t10.2097\t9.95932\t10.4852\t9.94327\t10.0768\t10.3195\t10.6859\t9.93363\t9.72816\t11.5058\t11.572\t10.9028\t9.60297\t10.2204\t9.85637\t10.4888\t10.0416\t10.046\t10.413\t10.7652\t7.52293\t8.70894\t9.20765\t10.8573\t10.5516\t10.3877\t8.87027\t10.1094\t10.8421\t9.4909\t10.2367\t11.5272\t9.97474\t9.55822\t11.7692\t11.0657\t11.6201\t9.41964\t9.63991\t11.2332\t10.9208\t9.37051\t11.9045\t9.07831\t10.3008\t10.6373\t9.23329\t11.0251\t10.9739\t8.6814\t10.517\t11.4875\t10.3289\t10.3869\t9.9029\t10.7938\t9.62422\t12.2829\t11.4593\t10.7166\t9.5399\t10.7338\t9.5919\t10.4037\t11.3321\t9.68459\t9.18517\t10.3003\t8.26042\t10.4932\t11.0899\t10.292\t7.58998\t11.2499\t10.7987\t10.4238\t10.3081\t9.64205\t10.1254\t9.90566\t9.28895\t9.87581\t10.222\t9.77873\t10.5928\t11.2688\t11.0928\t10.2429\t10.5264\t11.7399\t9.69309\t10.6458\t10.4584\t10.6425\t9.16025\t11.2781\t10.0396\t9.48063\t10.1367\t10.274\t10.1403\t10.8663\t11.3173\t11.02\t10.1939\t10.3503\t10.7752\t11.3636\t11.3725\t10.5294\t10.0056\t11.2646\t9.0807\t11.4224\t10.0095\t10.8749\t10.5853\t10.9375\t9.87489\t9.48708\t11.085\t10.3315\t9.5347\t9.3572\t10.864\t9.82683\t11.6873\t10.4075\t10.1797\t10.8484\t10.9478\t11.3358\t11.5244\t11.0379\t11.1631\t8.80613\t9.92719\t10.8618\n",
            "CCNB1\t9.55569\t9.96803\t10.0773\t11.3507\t10.9861\t10.7209\t10.073\t10.9533\t9.15213\t10.0455\t10.4081\t11.2916\t9.74768\t12.2432\t12.021\t10.3552\t10.1632\t9.64707\t9.67361\t10.3455\t10.243\t10.8738\t10.0511\t11.1879\t10.9137\t10.7746\t10.178\t10.0478\t10.8026\t10.876\t11.0838\t10.6406\t10.8806\t8.87476\t11.1235\t9.08678\t10.4129\t9.32852\t10.064\t11.2642\t9.30638\t10.3596\t10.1625\t10.7465\t10.7571\t10.4946\t11.2437\t10.9408\t11.3271\t10.7807\t10.6232\t9.61072\t10.1877\t10.8523\t10.3829\t10.7278\t11.1116\t10.9379\t10.1881\t10.0474\t10.0196\t11.2952\t10.9766\t10.6407\t10.8673\t10.6422\t9.28867\t9.62883\t11.5187\t10.8075\t10.9622\t10.2925\t10.2879\t9.57191\t10.4765\t10.805\t9.50566\t10.7906\t10.4111\t10.2977\t11.3483\t10.758\t10.8263\t11.5578\t11.2563\t10.9728\t9.65203\t10.4229\t9.04425\t11.1566\t11.4981\t10.2819\t9.97966\t9.10823\t10.7997\t11.2674\t10.9353\t9.58197\t8.8545\t8.70149\t10.1154\t8.39278\t9.01435\t10.1719\t9.39049\t9.61881\t9.72982\t9.80121\t8.87924\t8.8452\t10.4339\t8.9138\t10.7989\t10.1873\t8.83398\t8.38634\t10.6415\t9.16473\t9.95829\t9.93638\t8.86901\t10.0789\t11.5129\t8.30854\t9.75167\t9.49713\t7.9517\t9.25661\t8.53807\t9.38298\t10.2466\t8.71843\t9.6444\t8.44159\t9.62879\t10.2122\t9.13801\t10.5851\t9.0834\t10.0031\t8.30526\t9.62256\t9.50925\t9.84984\t8.47443\t9.7325\t9.55598\t9.59144\t9.98438\t8.13635\t10.2536\t8.99574\t9.27246\t9.73166\t9.84419\t8.72586\t10.1179\t8.74573\t9.19444\t8.47041\t8.81847\t8.83487\t8.96733\t8.74404\t8.71913\t7.59175\t8.23041\t8.89757\t9.25434\t8.85859\t9.10238\t8.63581\t7.89043\t10.0923\t9.99736\t8.89438\t9.93997\t10.7504\t9.53833\t10.2917\t9.54668\t10.3254\t7.86251\t8.87392\t8.34013\t8.27202\t9.39851\t9.11961\t11.0929\t7.92057\t8.70506\t8.99097\t8.68394\t9.22764\t9.88923\t9.26038\t9.31281\t8.97762\t9.39843\t9.74331\t8.89718\t10.0725\t7.52197\t7.43079\t10.1192\t9.54013\t8.44644\t9.78889\t9.50747\t9.65545\t9.74649\t8.09519\t10.2955\t10.1748\t10.1068\t10.4376\t9.12746\t9.52287\t10.0211\t9.54528\t10.0391\t9.83207\t7.78461\t8.69894\t9.50004\t9.49561\t9.93666\t9.02718\t8.59805\t9.88398\t10.323\t9.19993\t10.3259\t7.78529\t9.21109\t8.58298\t8.78012\t9.93472\t9.22007\t8.90489\t8.82474\t11.0259\t9.07449\t9.98391\t9.04231\t10.2659\t9.24804\t9.54415\t9.07419\t9.32982\t9.73334\t8.68835\t7.92882\t9.44968\t9.85543\t8.16955\t9.3488\t9.79761\t9.73148\t8.25699\t8.88198\t7.57066\t8.83758\t9.22059\t8.70307\t12.2045\t9.28215\t8.45039\t8.7379\t10.4738\t8.24371\t9.92637\t9.82464\t9.07516\t10.6248\t9.30755\t8.94301\t8.22014\t9.83849\t9.83326\t9.19656\t9.51334\t9.40727\t10.3488\t8.1741\t9.82379\t10.0846\t9.92296\t8.67924\t8.15563\t8.58559\t9.19001\t8.75264\t9.89678\t8.83413\t8.79118\t9.87606\t8.42181\t9.81916\t9.4886\t8.992\t10.2456\t8.11195\t9.88591\t8.61572\t10.4136\t8.87978\t10.1453\t10.0418\t8.9524\t9.03763\t11.038\t9.56368\t10.4296\t9.24095\t8.80302\t10.4235\t9.18681\t9.45785\t9.77996\t9.54306\t9.82875\t9.30499\t10.0103\t8.14069\t8.75316\t8.89394\t9.72648\n",
            "CCNE1\t7.03141\t6.39755\t10.0832\t10.1101\t9.6732\t7.38449\t8.17098\t9.11073\t6.65917\t9.97895\t8.24705\t9.69782\t9.5629\t8.90078\t8.5341\t10.349\t7.37111\t7.00667\t9.09773\t9.21204\t8.55335\t9.2309\t6.562\t9.59527\t9.5552\t7.36497\t8.56073\t8.65514\t8.92078\t8.4603\t9.16087\t10.6368\t8.64494\t5.21805\t9.69252\t8.39676\t7.34727\t8.8876\t6.27611\t9.21826\t7.07537\t7.8926\t9.80325\t8.26655\t9.25367\t8.98585\t9.18202\t9.43717\t9.02524\t8.91031\t5.91588\t6.6215\t9.97577\t6.88699\t7.50776\t8.14308\t9.52563\t8.36134\t6.47373\t8.46181\t8.18309\t9.48047\t8.72163\t8.28406\t8.60603\t8.12099\t5.81978\t6.96011\t9.7678\t9.30447\t10.6905\t7.44894\t7.78755\t7.86061\t8.61881\t8.15099\t7.62642\t8.1786\t8.87577\t7.97215\t10.4675\t8.44572\t11.124\t8.8092\t8.54384\t7.75553\t7.23679\t8.46079\t6.326\t9.55492\t8.81642\t7.83856\t10.602\t6.9341\t9.38029\t10.7206\t9.42793\t5.66397\t6.69544\t5.05363\t5.96479\t4.77027\t5.45082\t5.51777\t4.89763\t6.36623\t5.56451\t6.4862\t4.72009\t5.10117\t5.10911\t5.7168\t7.19838\t6.30878\t5.71577\t4.49488\t6.54762\t4.99157\t5.92708\t5.84717\t4.6131\t6.72795\t11.7661\t4.20799\t5.36922\t5.92775\t3.69222\t3.71126\t3.80601\t5.12393\t5.53865\t5.89378\t5.10642\t4.22367\t5.40345\t6.65821\t6.07763\t8.01116\t5.21189\t6.25877\t4.65286\t4.83602\t4.84238\t5.69662\t4.00116\t4.95105\t4.80018\t4.62342\t6.27865\t4.57998\t6.80276\t3.26725\t5.4781\t5.30159\t4.73348\t7.40063\t6.46692\t4.15127\t5.76556\t4.75476\t3.15877\t5.64285\t4.75848\t5.22186\t6.20413\t4.35923\t4.38734\t4.71811\t5.15933\t5.11545\t4.57133\t4.48459\t4.1462\t6.06872\t5.9277\t4.4569\t6.9557\t6.55242\t5.6706\t5.39075\t4.05846\t5.68106\t3.39749\t4.5468\t5.75524\t5.00655\t6.6396\t6.16006\t6.30561\t4.09473\t6.24152\t5.25802\t4.91903\t5.46503\t5.57187\t5.89633\t4.40711\t5.33508\t6.61821\t4.68111\t3.97904\t5.2172\t3.46584\t4.21287\t5.89854\t5.85797\t3.6058\t5.10437\t4.31284\t4.58099\t4.7292\t4.96874\t7.51986\t10.4133\t7.1428\t5.51814\t4.98146\t5.36863\t5.90569\t5.0623\t4.20301\t5.76091\t3.70862\t5.21253\t4.95894\t5.10942\t4.71365\t5.18295\t4.98446\t5.93029\t5.55872\t5.77582\t7.1677\t5.01348\t5.78322\t4.05054\t5.11853\t5.70751\t5.17584\t4.32871\t4.82834\t6.01359\t4.538\t5.45469\t3.88095\t5.06239\t5.12691\t4.64889\t5.13313\t5.35244\t5.72979\t6.47606\t3.90811\t4.98468\t5.75136\t5.7078\t4.20196\t6.42477\t5.58647\t5.25215\t4.39626\t5.56089\t3.07897\t4.66228\t5.21541\t6.58597\t6.53815\t4.21723\t4.64717\t6.02892\t3.54102\t5.34253\t5.80388\t4.79567\t5.76388\t5.58445\t4.18133\t4.9593\t6.1944\t5.56059\t5.79042\t5.83487\t4.85798\t5.34131\t4.01491\t5.10154\t5.93101\t6.6337\t4.72214\t4.56094\t4.19333\t6.16133\t3.64101\t6.30939\t5.04595\t6.00106\t5.05677\t3.70971\t5.62943\t5.33956\t5.16193\t6.60763\t4.29012\t4.59398\t5.50089\t5.93928\t5.75265\t6.51188\t6.71389\t5.99982\t5.04174\t6.25969\t5.45653\t8.35498\t4.4067\t5.45721\t5.90728\t4.58228\t5.29919\t4.91112\t3.97\t5.21832\t6.92442\t4.64723\t3.80524\t3.54125\t4.95567\t4.50262\n",
            "CDC6\t8.44544\t8.40945\t9.02709\t8.82195\t9.38211\t8.23001\t9.34852\t10.6887\t7.73368\t9.47992\t8.72074\t9.93116\t8.65318\t9.07374\t9.55395\t11.2895\t8.96023\t9.39701\t8.51746\t9.62058\t9.50962\t9.20595\t8.66569\t10.3204\t9.64606\t10.6997\t8.82622\t7.75713\t8.96867\t9.73557\t9.26018\t9.00303\t8.91576\t6.56209\t9.72715\t9.74055\t9.67423\t8.03053\t10.6514\t9.79693\t9.12199\t9.76745\t9.6461\t8.9212\t9.61173\t10.3057\t8.9841\t9.9172\t10.0803\t10.612\t7.94251\t8.87917\t9.82494\t9.54404\t8.85144\t8.88276\t8.76277\t9.84804\t8.14848\t8.6585\t9.64764\t9.29125\t9.12436\t9.28383\t9.80507\t10.2471\t7.50413\t8.38833\t10.1531\t8.85251\t10.0611\t9.26907\t9.23299\t9.21945\t8.77729\t10.7299\t9.44625\t10.6362\t9.42583\t9.83869\t9.41399\t9.0183\t9.501\t9.21065\t9.67999\t9.75052\t9.32579\t8.53015\t7.67986\t9.79778\t9.63738\t9.13838\t9.28993\t8.99903\t10.1959\t9.14087\t9.54891\t9.20692\t8.1889\t7.82326\t7.92338\t7.14397\t7.86743\t8.60461\t8.45836\t8.76351\t8.34858\t8.89431\t7.69915\t7.58958\t9.27086\t7.4354\t9.20005\t8.97507\t7.63894\t6.71964\t9.08121\t7.13995\t7.64271\t9.4833\t7.33495\t9.38546\t10.2052\t6.30754\t8.18176\t8.43854\t6.32748\t7.33849\t6.63783\t6.44837\t8.26552\t7.8092\t8.98118\t5.74519\t9.12879\t8.14866\t7.93865\t8.49319\t7.9407\t7.48493\t6.6234\t7.85857\t7.3362\t7.50067\t7.45038\t6.73127\t7.78459\t7.8896\t7.46662\t6.55\t8.65153\t6.11191\t8.91823\t7.39964\t7.84811\t6.73883\t6.88547\t7.87302\t8.33314\t5.9904\t5.55454\t7.82224\t7.74366\t7.70339\t8.20651\t6.29681\t9.12664\t7.1467\t7.3141\t7.32579\t6.88868\t7.30424\t5.7839\t9.24275\t8.58398\t6.98146\t9.80984\t8.46571\t8.30035\t9.68191\t8.60901\t8.72542\t5.58442\t6.15982\t8.97583\t6.81751\t9.27457\t7.6704\t8.68619\t6.08797\t10.1617\t8.45721\t7.18449\t7.36333\t6.96239\t8.08798\t8.33302\t6.55666\t8.4921\t8.04621\t7.74922\t8.3638\t5.65215\t6.52306\t9.12094\t8.76408\t5.24648\t9.1531\t8.05861\t8.77846\t7.76134\t6.73045\t9.58948\t8.08876\t8.93932\t8.81637\t7.34034\t8.01443\t8.87714\t7.54646\t8.23851\t7.95288\t5.26528\t7.74652\t8.36428\t6.8518\t8.39124\t7.75496\t7.72634\t8.37205\t8.79333\t7.8376\t8.47858\t5.56088\t8.78472\t7.27712\t8.22014\t9.31417\t6.97421\t8.19858\t6.6544\t8.36645\t7.28509\t9.25082\t5.95996\t9.39953\t7.73345\t8.56412\t6.79515\t8.63568\t8.67786\t8.52302\t5.46963\t9.48659\t9.29974\t8.43727\t7.6146\t8.92322\t9.30541\t6.22345\t6.93641\t5.9156\t6.57713\t7.72966\t7.35092\t8.15182\t8.47366\t6.98413\t7.7767\t8.34194\t6.55241\t8.10553\t7.715\t7.37807\t7.97602\t7.24443\t6.79947\t7.19564\t7.60879\t8.27555\t8.04124\t6.5597\t8.10329\t7.44288\t8.28273\t8.98174\t8.00861\t11.1256\t7.07655\t6.78042\t7.06167\t8.63433\t6.81871\t8.36821\t7.12185\t7.25473\t8.29291\t7.32237\t8.7345\t8.09264\t8.2676\t8.53907\t6.56789\t7.54852\t8.98004\t10.5222\t7.79148\t8.17618\t8.5189\t7.55889\t7.37474\t9.23746\t8.04567\t8.94926\t8.16836\t7.30618\t9.05938\t6.91218\t8.68915\t8.59034\t7.87709\t8.47443\t14.1107\t8.15038\t6.47995\t6.69548\t7.76166\t7.51795\n",
            "  sample_id TCGA-A7-A13E-01A-11R-A12P-07 TCGA-A8-A08H-01A-21R-A00Z-07  \\\n",
            "0     class                   Basal-like                   Basal-like   \n",
            "1      NAT1                      3.59727                       7.2052   \n",
            "2     BIRC5                      8.38208                      8.53294   \n",
            "3      BAG1                      11.0157                      10.3315   \n",
            "4      BCL2                      10.3142                      9.25682   \n",
            "\n",
            "  TCGA-A8-A07U-01A-11R-A034-07 TCGA-BH-A0E0-01A-11R-A056-07  \\\n",
            "0                   Basal-like                   Basal-like   \n",
            "1                      5.12902                      4.82895   \n",
            "2                      9.01439                      8.74525   \n",
            "3                      10.5027                      10.5346   \n",
            "4                      8.77254                      8.37459   \n",
            "\n",
            "  TCGA-A2-A04T-01A-21R-A034-07 TCGA-BH-A18K-01A-11R-A12D-07  \\\n",
            "0                   Basal-like                   Basal-like   \n",
            "1                      4.79575                      10.1133   \n",
            "2                      10.1728                      8.38562   \n",
            "3                      9.39239                      10.2508   \n",
            "4                      8.62946                      9.31121   \n",
            "\n",
            "  TCGA-AQ-A04J-01A-02R-A034-07 TCGA-A1-A0SK-01A-12R-A084-07  \\\n",
            "0                   Basal-like                   Basal-like   \n",
            "1                      6.60699                      2.59515   \n",
            "2                       9.3852                      9.40988   \n",
            "3                      10.0683                      10.4644   \n",
            "4                      9.41709                       12.361   \n",
            "\n",
            "  TCGA-A7-A0DA-01A-31R-A115-07  ... TCGA-B6-A0IP-01A-11R-A034-07  \\\n",
            "0                   Basal-like  ...                    Luminal A   \n",
            "1                      6.09316  ...                      12.9465   \n",
            "2                      9.26187  ...                      7.82759   \n",
            "3                       10.608  ...                      10.7754   \n",
            "4                      8.09499  ...                      11.7466   \n",
            "\n",
            "  TCGA-A8-A09T-01A-11R-A00Z-07 TCGA-AN-A03X-01A-21R-A00Z-07  \\\n",
            "0                    Luminal A                    Luminal A   \n",
            "1                      10.1884                      13.1863   \n",
            "2                      7.31434                      8.34027   \n",
            "3                      10.4413                      9.73608   \n",
            "4                      11.3728                      9.38138   \n",
            "\n",
            "  TCGA-BH-A0HF-01A-11R-A056-07 TCGA-B6-A0IG-01A-11R-A034-07  \\\n",
            "0                    Luminal A                    Luminal A   \n",
            "1                      12.2478                      11.2064   \n",
            "2                      8.30837                      8.07246   \n",
            "3                      10.5251                      11.5608   \n",
            "4                      11.2281                      10.3105   \n",
            "\n",
            "  TCGA-AN-A0XV-01A-11R-A109-07 TCGA-B6-A0RP-01A-21R-A084-07  \\\n",
            "0                    Luminal A                    Luminal A   \n",
            "1                      10.4554                      13.6232   \n",
            "2                      7.66668                      6.15863   \n",
            "3                      10.7085                       10.281   \n",
            "4                      12.0191                       10.781   \n",
            "\n",
            "  TCGA-B6-A0X7-01A-11R-A10J-07 TCGA-BH-A0HK-01A-11R-A056-07  \\\n",
            "0                    Luminal A                    Luminal A   \n",
            "1                      10.5333                      11.9222   \n",
            "2                      6.69209                      8.26247   \n",
            "3                      10.5999                      10.6069   \n",
            "4                      11.8249                       11.319   \n",
            "\n",
            "  TCGA-A2-A0ET-01A-31R-A034-07  \n",
            "0                    Luminal A  \n",
            "1                      13.7303  \n",
            "2                      7.38246  \n",
            "3                       10.661  \n",
            "4                      11.9105  \n",
            "\n",
            "[5 rows x 329 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Perform the necessary parsing to get what you need in order"
      ],
      "metadata": {
        "id": "pziiNRVx-0BA"
      }
    },
    {
      "source": [
        "# Extract features (gene expression values) and target classes (subtype)\n",
        "# Samples are all columns except the first one (which contains gene names)\n",
        "# Features are all rows except the first two (that contain sample ID and class)\n",
        "X = data.iloc[1:, 1:].T.values  # Features (gene expression values)\n",
        "y = data.iloc[0, 1:].values  # Target classes\n",
        "print(X)\n",
        "print(y)\n",
        "# Convert target labels to numerical values (0 for Basal-like, 1 for Luminal A)\n",
        "y = [0 if label == 'Basal-like' else 1 for label in y]\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQBgdhdXKBbP",
        "outputId": "5d7878d0-6a55-475c-9033-1fa68b08caec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['3.59727' '8.38208' '11.0157' ... '3.37778' '9.02075' '4.2101']\n",
            " ['7.2052' '8.53294' '10.3315' ... '9.81946' '7.53126' '5.43699']\n",
            " ['5.12902' '9.01439' '10.5027' ... '7.64791' '10.4528' '5.38517']\n",
            " ...\n",
            " ['10.5333' '6.69209' '10.5999' ... '13.2352' '6.5445' '7.72875']\n",
            " ['11.9222' '8.26247' '10.6069' ... '13.8731' '7.21966' '9.78624']\n",
            " ['13.7303' '7.38246' '10.661' ... '13.8056' '7.82931' '7.21018']]\n",
            "['Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like' 'Basal-like'\n",
            " 'Basal-like' 'Basal-like' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A'\n",
            " 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A' 'Luminal A']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Use logistic regression to learn and asses classification models within cross-validation"
      ],
      "metadata": {
        "id": "b4hLjWRCWYgk"
      }
    },
    {
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings from logistic regression\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.linear_model\")\n",
        "\n",
        "\n",
        "# Test logistic regression with a simple split into training and test sets\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Test logistic regression using cross-validation\n",
        "# Perform 5-fold cross-validation\n",
        "\n",
        "# Create a StratifiedKFold object with 5 splits\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform stratified cross-validation\n",
        "model = LogisticRegression()\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Compute mean accuracy and standard deviation\n",
        "mean_accuracy = np.mean(scores)\n",
        "std_accuracy = np.std(scores)\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
        "print(f\"Standard Deviation: {std_accuracy}\")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CLB0cpwQ0iB",
        "outputId": "6bfd10db-349e-4e61-a952-8724373d6e89"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9545454545454546\n",
            "Mean Accuracy: 0.9663403263403264\n",
            "Standard Deviation: 0.022574214151553078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Use a simple feed-forward neural network to train and assess classification models within cross-validation"
      ],
      "metadata": {
        "id": "-srSq031aRIL"
      }
    },
    {
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.neural_network\")\n",
        "\n",
        "\n",
        "# Create a StratifiedKFold object with 5 splits\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Create and train a neural network model with one hidden layer (10 nodes)\n",
        "model = MLPClassifier(hidden_layer_sizes=(10,), activation='relu', solver='adam', random_state=42)\n",
        "\n",
        "# Perform stratified cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Compute mean accuracy and standard deviation\n",
        "mean_accuracy = np.mean(scores)\n",
        "std_accuracy = np.std(scores)\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
        "print(f\"Standard Deviation: {std_accuracy}\")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nTEBQ2HXJO9",
        "outputId": "ac221d7a-103c-4504-9c52-2558e76010a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.9785547785547786\n",
            "Standard Deviation: 0.018415803584402812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen through the mean accuracy values, neural networks perform better than simple logistic regression based classification to categorize luminal A vs basal cancer subtypes. As can be seen through a lower standard deviation value for the neural network model, it is more stable across the folds."
      ],
      "metadata": {
        "id": "1IgI-nRkMWVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Play with meta-parameters in the model, including different activation functions, more hidden layers, more or less nodes in hidden layers to change the complexity of the model and its impact on the results."
      ],
      "metadata": {
        "id": "Hj3-jJwqCwPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.neural_network\")\n",
        "\n",
        "# Cross-validation setup\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "#Code written with the help of ChatGPT\n",
        "# Different model configs to try\n",
        "configs = [\n",
        "    {\"hidden_layer_sizes\": (10,), \"activation\": \"relu\"},\n",
        "\n",
        "    #increasing the number of nodes\n",
        "    {\"hidden_layer_sizes\": (50,), \"activation\": \"relu\"},\n",
        "    {\"hidden_layer_sizes\": (100,), \"activation\": \"relu\"},\n",
        "\n",
        "    #adding more hidden layers with varying number of nodes and different activation functions\n",
        "    {\"hidden_layer_sizes\": (50, 25), \"activation\": \"relu\"},\n",
        "    {\"hidden_layer_sizes\": (30, 30, 30), \"activation\": \"tanh\"},\n",
        "    {\"hidden_layer_sizes\": (64, 32), \"activation\": \"logistic\"},\n",
        "    {\"hidden_layer_sizes\": (100, 50, 10), \"activation\": \"relu\"},\n",
        "]\n",
        "\n",
        "# Run cross-validation for each config\n",
        "for idx, config in enumerate(configs):\n",
        "    model = MLPClassifier(\n",
        "        hidden_layer_sizes=config[\"hidden_layer_sizes\"],\n",
        "        activation=config[\"activation\"],\n",
        "        solver='adam',\n",
        "        max_iter=500,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "    mean_accuracy = np.mean(scores)\n",
        "    std_accuracy = np.std(scores)\n",
        "\n",
        "    print(f\"\\nModel {idx + 1}: hidden_layers={config['hidden_layer_sizes']}, activation={config['activation']}\")\n",
        "    print(f\"Mean Accuracy: {mean_accuracy:.4f}\")\n",
        "    print(f\"Standard Deviation: {std_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9g2Uzn7MOzs",
        "outputId": "e66eebb1-36bf-4ebb-9779-3df85915bb86"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model 1: hidden_layers=(10,), activation=relu\n",
            "Mean Accuracy: 0.9786\n",
            "Standard Deviation: 0.0184\n",
            "\n",
            "Model 2: hidden_layers=(50,), activation=relu\n",
            "Mean Accuracy: 0.9725\n",
            "Standard Deviation: 0.0150\n",
            "\n",
            "Model 3: hidden_layers=(100,), activation=relu\n",
            "Mean Accuracy: 0.9786\n",
            "Standard Deviation: 0.0184\n",
            "\n",
            "Model 4: hidden_layers=(50, 25), activation=relu\n",
            "Mean Accuracy: 0.9755\n",
            "Standard Deviation: 0.0208\n",
            "\n",
            "Model 5: hidden_layers=(30, 30, 30), activation=tanh\n",
            "Mean Accuracy: 0.9724\n",
            "Standard Deviation: 0.0225\n",
            "\n",
            "Model 6: hidden_layers=(64, 32), activation=logistic\n",
            "Mean Accuracy: 0.9786\n",
            "Standard Deviation: 0.0184\n",
            "\n",
            "Model 7: hidden_layers=(100, 50, 10), activation=relu\n",
            "Mean Accuracy: 0.9724\n",
            "Standard Deviation: 0.0180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance of the model remains robust to changes in the parameters like number of hidden layers, the number of nodes in each of the hidden layers and also the type of activation function used. This suggests that adding complexity to this model does not necessarily improve generalization since this dataset is relatively easy to classify. The best model was the one with one hidden layer and 100 nodes as well as the one with two hidden layers with 64 and 32 nodes respectively. This is the architecture I am going to use for the next part of the project considering the fact that the ER status dataset is larger and more complicated."
      ],
      "metadata": {
        "id": "u5WNIGaLPmV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Note that Basal-like vs. Luminal A subtype classification is an easy task as these two classes represent two most distinct molecular subtypes of breast cancer. Note also that the problem is further simplified by using an oracle provided magic set of 50 features (genes) that separate these subtypes very well. Following Project 1, expand the code to perform both logistic regression and neural network training and assessment within cross-validation on the more challenging task of predicting ER status using all genes with t-test based feature selection to limit the set of genes to 1000, 500, 100, and compare the results. Make sure that your feature selection is done on the training subsets only!"
      ],
      "metadata": {
        "id": "gkZimdRfapb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading the file manually since colab was not connecting to google drive\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "v9aicU6fYr9z",
        "outputId": "bd7872df-80b5-4a52-bd2a-903acb3e53fe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a45b28f2-8717-4069-a43f-7f17b4d1d4d2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a45b28f2-8717-4069-a43f-7f17b4d1d4d2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TCGA_breast_cancer_ERstatus_allGenes.txt to TCGA_breast_cancer_ERstatus_allGenes.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "#Load the dataset\n",
        "data = pd.read_csv(\"TCGA_breast_cancer_ERstatus_allGenes.txt\", sep=\"\\t\")\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLM4TvC4Yvmm",
        "outputId": "9aa601db-96fd-4555-f4cc-3b016f114676"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-8c5f881deb1b>:4: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(\"TCGA_breast_cancer_ERstatus_allGenes.txt\", sep=\"\\t\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               id TCGA-A7-A13E-01A-11R-A12P-07 TCGA-A8-A08H-01A-21R-A00Z-07  \\\n",
            "0  class-ERstatus                     Positive                     Positive   \n",
            "1            A1BG                  6.862111631                  7.194049613   \n",
            "2             A2M                  12.27289156                  15.28033619   \n",
            "3            NAT1                  3.597268951                  7.205198947   \n",
            "4            NAT2                  0.512985335                   2.56078831   \n",
            "\n",
            "  TCGA-A8-A07U-01A-11R-A034-07 TCGA-BH-A0E0-01A-11R-A056-07  \\\n",
            "0                     Negative                     Negative   \n",
            "1                   4.40083163                  5.849819161   \n",
            "2                  13.94199511                  12.83033137   \n",
            "3                  5.129023308                  4.828951401   \n",
            "4                  0.485323774                  3.848647976   \n",
            "\n",
            "  TCGA-A2-A04T-01A-21R-A034-07 TCGA-BH-A18K-01A-11R-A12D-07  \\\n",
            "0                     Negative                     Positive   \n",
            "1                  6.055269419                  8.435080563   \n",
            "2                  13.61798149                  13.32269943   \n",
            "3                  4.795746171                  10.11333117   \n",
            "4                   1.25755379                  4.428953043   \n",
            "\n",
            "  TCGA-AQ-A04J-01A-02R-A034-07 TCGA-A1-A0SK-01A-12R-A084-07  \\\n",
            "0                     Negative                     Negative   \n",
            "1                  6.058647689                   6.45357684   \n",
            "2                   16.2010389                  12.03320138   \n",
            "3                  6.606994386                  2.595145568   \n",
            "4                  2.357749042                            0   \n",
            "\n",
            "  TCGA-A7-A0DA-01A-31R-A115-07  ... TCGA-B6-A0IP-01A-11R-A034-07  \\\n",
            "0                     Negative  ...                     Positive   \n",
            "1                  6.682105525  ...                  6.501224158   \n",
            "2                  13.20493465  ...                  12.83295024   \n",
            "3                  6.093160828  ...                  12.94651302   \n",
            "4                   2.94362132  ...                  2.454623375   \n",
            "\n",
            "  TCGA-A8-A09T-01A-11R-A00Z-07 TCGA-AN-A03X-01A-21R-A00Z-07  \\\n",
            "0                     Positive                     Positive   \n",
            "1                  7.702271055                  8.154039856   \n",
            "2                  12.57663215                  12.83443599   \n",
            "3                  10.18844111                   13.1863253   \n",
            "4                  3.383220433                  4.829230549   \n",
            "\n",
            "  TCGA-BH-A0HF-01A-11R-A056-07 TCGA-B6-A0IG-01A-11R-A034-07  \\\n",
            "0                     Positive                     Positive   \n",
            "1                  6.031340053                  7.899675338   \n",
            "2                  14.36302802                   12.5988649   \n",
            "3                  12.24781266                  11.20641368   \n",
            "4                   1.69434535                  1.342896037   \n",
            "\n",
            "  TCGA-AN-A0XV-01A-11R-A109-07 TCGA-B6-A0RP-01A-21R-A084-07  \\\n",
            "0                     Positive                     Positive   \n",
            "1                   6.83075223                  7.486263863   \n",
            "2                   14.2587428                  12.78688385   \n",
            "3                   10.4553974                  13.62319105   \n",
            "4                  3.684257705                  2.247806018   \n",
            "\n",
            "  TCGA-B6-A0X7-01A-11R-A10J-07 TCGA-BH-A0HK-01A-11R-A056-07  \\\n",
            "0                     Positive                     Positive   \n",
            "1                  7.928911528                   4.42947537   \n",
            "2                  13.84074778                  14.39031168   \n",
            "3                  10.53327862                  11.92216573   \n",
            "4                  3.360195695                  1.861717211   \n",
            "\n",
            "  TCGA-A2-A0ET-01A-31R-A034-07  \n",
            "0                     Positive  \n",
            "1                  7.039138394  \n",
            "2                  12.39786509  \n",
            "3                  13.73026968  \n",
            "4                  8.095296796  \n",
            "\n",
            "[5 rows x 324 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the target labels (ER status) from the first row (excluding the 'id' column)\n",
        "y = data.iloc[0, 1:].values  # ER status for each sample\n",
        "\n",
        "# Extracting gene expression values from all rows except the first (gene names)\n",
        "X = data.iloc[1:, 1:].T.values  # Gene expression values (features)\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "# Convert ER status labels to binary: 1 for Positive, 0 for Negative\n",
        "y = [1 if label == 'Positive' else 0 for label in y]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV07vEEaY26e",
        "outputId": "45df7269-e368-4db5-c06b-d155eb2d95eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['6.862111631' '12.27289156' '3.597268951' ... 2.14280551 4.711208542\n",
            "  4.824258697]\n",
            " ['7.194049613' '15.28033619' '7.205198947' ... 0.919149363 5.008267255\n",
            "  4.127872609]\n",
            " ['4.40083163' '13.94199511' '5.129023308' ... 1.378345149 4.666500974\n",
            "  3.743913448]\n",
            " ...\n",
            " ['7.928911528' '13.84074778' '10.53327862' ... 3.498084884 6.473817752\n",
            "  4.931815408]\n",
            " ['4.42947537' '14.39031168' '11.92216573' ... 2.648166289 5.658948355\n",
            "  4.39376626]\n",
            " ['7.039138394' '12.39786509' '13.73026968' ... 3.055386566 4.918977694\n",
            "  5.620949971]]\n",
            "['Positive' 'Positive' 'Negative' 'Negative' 'Negative' 'Positive'\n",
            " 'Negative' 'Negative' 'Negative' 'Negative' 'Negative' 'Negative'\n",
            " 'Negative' 'Positive' 'Negative' 'Negative' 'Negative' 'Negative'\n",
            " 'Negative' 'Negative' 'Negative' 'Negative' 'Negative' 'Negative'\n",
            " 'Negative' 'Negative' 'Negative' 'Negative' 'Negative' 'Negative'\n",
            " 'Negative' 'Positive' 'Negative' 'Positive' 'Negative' 'Negative'\n",
            " 'Negative' 'Negative' 'Negative' 'Negative' 'Negative' 'Negative'\n",
            " 'Negative' 'Negative' 'Negative' 'Positive' 'Negative' 'Positive'\n",
            " 'Negative' 'Negative' 'Negative' 'Negative' 'Negative' 'Positive'\n",
            " 'Negative' 'Negative' 'Negative' 'Negative' 'Positive' 'Negative'\n",
            " 'Negative' 'Negative' 'Negative' 'Negative' 'Positive' 'Negative'\n",
            " 'Negative' 'Negative' 'Negative' 'Negative' 'Negative' 'Negative'\n",
            " 'Negative' 'Negative' 'Positive' 'Negative' 'Negative' 'Negative'\n",
            " 'Negative' 'Negative' 'Negative' 'Negative' 'Negative' 'Negative'\n",
            " 'Negative' 'Negative' 'Negative' 'Positive' 'Negative' 'Negative'\n",
            " 'Negative' 'Positive' 'Negative' 'Negative' 'Negative' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Negative' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Negative'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Negative'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Negative' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Negative' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Negative' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Negative' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
            " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGISTIC REGRESSION TO CLASSIFY SAMPLES AS ER+/ER-"
      ],
      "metadata": {
        "id": "zwolGdHunQRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy import stats\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings from logistic regression (e.g., convergence warnings)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.linear_model\")\n",
        "\n",
        "# Ensure X and y are numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Define different numbers of top genes to test\n",
        "top_gene_counts = [1000, 500, 100]\n",
        "\n",
        "# Iterate over each top_gene threshold\n",
        "for top_genes in top_gene_counts:\n",
        "    print(f\"\\nEvaluating for Top {top_genes} genes\")\n",
        "\n",
        "    # Store accuracy scores for this gene count\n",
        "    cv_accuracies = []\n",
        "\n",
        "    # Test logistic regression using cross-validation\n",
        "    # Perform 5-fold cross-validation\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        # Split data\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # Perform t-test for each gene\n",
        "        p_values = []\n",
        "        for gene_idx in range(X_train.shape[1]):\n",
        "            try:\n",
        "                _, p = stats.ttest_ind(X_train[y_train == 0, gene_idx], X_train[y_train == 1, gene_idx], equal_var=False)\n",
        "            except:\n",
        "                p = 1.0  # Assign high p-value if test fails\n",
        "            p_values.append(p)\n",
        "\n",
        "        # Select top genes\n",
        "        top_gene_indices = np.argsort(p_values)[:top_genes]\n",
        "        X_train_selected = X_train[:, top_gene_indices]\n",
        "        X_test_selected = X_test[:, top_gene_indices]\n",
        "\n",
        "        # Train and evaluate model\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "        model.fit(X_train_selected, y_train)\n",
        "        y_pred = model.predict(X_test_selected)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        cv_accuracies.append(acc)\n",
        "\n",
        "\n",
        "    # Compute mean accuracy and standard deviation\n",
        "    mean_acc = np.mean(cv_accuracies)\n",
        "    std_acc = np.std(cv_accuracies)\n",
        "    print(f\"Mean Accuracy: {mean_acc:.4f}\")\n",
        "    print(f\"Standard Deviation: {std_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JZKTClNjO6q",
        "outputId": "5e6e6374-e0b2-4d95-99ff-a1966b1913fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating for Top 1000 genes\n",
            "Mean Accuracy: 0.9505\n",
            "Standard Deviation: 0.0149\n",
            "\n",
            "Evaluating for Top 500 genes\n",
            "Mean Accuracy: 0.9350\n",
            "Standard Deviation: 0.0117\n",
            "\n",
            "Evaluating for Top 100 genes\n",
            "Mean Accuracy: 0.9318\n",
            "Standard Deviation: 0.0235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEURAL NETWORK MODEL TO CLASSIFY SAMPLES AS ER+/ER-"
      ],
      "metadata": {
        "id": "EX0EFCeanU6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define different numbers of top genes to test\n",
        "top_gene_counts = [1000, 500, 100]\n",
        "\n",
        "# Iterate over each top_gene threshold\n",
        "for top_genes in top_gene_counts:\n",
        "    print(f\"\\nEvaluating for Top {top_genes} genes\")\n",
        "\n",
        "    # Store accuracy scores for this gene count\n",
        "    cv_accuracies = []\n",
        "\n",
        "    # Test logistic regression using cross-validation\n",
        "    # Perform 5-fold cross-validation\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        # Split data\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # Perform t-test for each gene\n",
        "        p_values = []\n",
        "        for gene_idx in range(X_train.shape[1]):\n",
        "            try:\n",
        "                _, p = stats.ttest_ind(X_train[y_train == 0, gene_idx], X_train[y_train == 1, gene_idx], equal_var=False)\n",
        "            except:\n",
        "                p = 1.0  # Assign high p-value if test fails\n",
        "            p_values.append(p)\n",
        "\n",
        "        # Select top genes\n",
        "        top_gene_indices = np.argsort(p_values)[:top_genes]\n",
        "        X_train_selected = X_train[:, top_gene_indices]\n",
        "        X_test_selected = X_test[:, top_gene_indices]\n",
        "\n",
        "        # Define the neural network model\n",
        "        model = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', random_state=42)\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train_selected, y_train)\n",
        "\n",
        "        # Predict and evaluate\n",
        "        y_pred = model.predict(X_test_selected)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        cv_accuracies.append(acc)\n",
        "\n",
        "    # Compute mean and std of accuracies\n",
        "    mean_acc = np.mean(cv_accuracies)\n",
        "    std_acc = np.std(cv_accuracies)\n",
        "\n",
        "    print(f\"Mean Accuracy: {mean_acc:.4f}\")\n",
        "    print(f\"Standard Deviation: {std_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaMOsS_YnPRS",
        "outputId": "82ebec4a-3214-4cfc-caa1-281f6d00e755"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating for Top 1000 genes\n",
            "Mean Accuracy: 0.9443\n",
            "Standard Deviation: 0.0186\n",
            "\n",
            "Evaluating for Top 500 genes\n",
            "Mean Accuracy: 0.9381\n",
            "Standard Deviation: 0.0097\n",
            "\n",
            "Evaluating for Top 100 genes\n",
            "Mean Accuracy: 0.9320\n",
            "Standard Deviation: 0.0154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression and the neural network model perform pretty similarly as can be depicted by the mean accuracy values and their standard deviation. Both the logisitc regression model and the neural network model perform the best with top features = 1000. This is because with fewer features, some informative genes are excluded. The best model here is the logistic regression model with top selected features = 1000. The neural networks display a slightly more consistent performance and also perform slightly better than logistic regression when it comes to classifying samples as ER+/ER- with only 100 features. I also believe a neural network model with a more complex architecture will outperform the logistic regression model for the same number of features. Additionally, we can further analyze whether the models are underfitting/overfitting by analzying the training vs validation accuracy and printing learning curves for both training and validation."
      ],
      "metadata": {
        "id": "9wEQ9vcvw-pr"
      }
    }
  ]
}