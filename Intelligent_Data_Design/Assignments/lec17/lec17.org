#+title: lec17

* 1.
#+name: data
| ID |  y | haty |
|----+----+------|
|  1 |  1 |   -1 |
|  2 | -1 |    1 |
|  3 | -1 |    1 |
|  4 |  1 |   -1 |
|  5 | -1 |    1 |
|  6 |  1 |    1 |
|  7 | -1 |   -1 |
|  8 |  1 |    1 |
#+tblfm: $1=@#-1

** a.
I went ahead and computed the values anyway with python:
#+begin_src python :var data=data
right = 0
for i in data:
    if i[1] == i[2]:
        right += 1
accuracy = right/len(data)
error_rate = 1 - accuracy
out = [["accuracy:", accuracy],["error rate:", error_rate]]
return out
#+end_src

#+RESULTS:
| accuracy:   | 0.375 |
| error rate: | 0.625 |
** b
|           | True |    |
|-----------+------+----|
| predicted |    1 | -1 |
|-----------+------+----|
|         1 |    2 |  2 |
|        -1 |    3 |  1 |
+ True positive: 2
+ False positive: 2
+ True negative: 1
+ false negative: 3
** c, d
+ Sensitivity = true positive rate = 2/5
+ Specificity = true negative rate = 1/3

The classifier does not have very good performance. It is more likely to return
a false positive or false negative than a true positive or negative.
** e.
+ Accuracy = precision = (2/(2+2)) = 2/4
+ recall = 2/(2+3) = 2/5
The accuracy and recall are very far from 1, this is a bad classifier.
** f.
\(F1 = \frac{2 \cdot pred \cdot recall}{pred + recall} = \frac{8/20}{18/20} = 1/2\)
* 2.
** a-b
#+caption: table 2
#+name: t1
| ID |  y |   p | p >= 0.8 | p >= 0.5 |
|----+----+-----+----------+----------|
|  1 | -1 | 0.9 |       -1 |       -1 |
|  2 |  1 | 0.8 |        1 |       -1 |
|  3 | -1 | 0.5 |        1 |        1 |
|  4 |  1 | 0.3 |        1 |        1 |
#+tblfm: $1=@#-1
#+tblfm: $4='(if (<= $3 0.8) 1 -1 );N
#+tblfm: $5='(if (<= $3 0.5) 1 -1 );N

#+caption: table 2
#+name: t2
| ID |  y |   p | p >= 0.8 | p >= 0.5 |
|----+----+-----+----------+----------|
|  1 | -1 | 0.5 |        1 |        1 |
|  2 |  1 | 0.8 |        1 |       -1 |
|  3 | -1 | 0.9 |       -1 |       -1 |
|  4 |  1 | 0.7 |        1 |       -1 |
#+tblfm: $1=@#-1
#+tblfm: $4='(if (<= $3 0.8) 1 -1 );N
#+tblfm: $5='(if (<= $3 0.5) 1 -1 );N

#+name: bayes
#+begin_src python :var table1=t1 :var table2=t2
def true_false(name, data):
    true_positives_8 = 0
    false_positives_8 = 0
    true_positives_5 = 0
    false_positives_5 = 0
    for i in data:
        if i[1] ==1:
            if i[3] == 1:
                true_positives_8 += 1
            else:
                false_positives_8 += 1
            if i[4] == 1:
                true_positives_5 += 1
            else:
                false_positives_5 += 1
    def recall(r1 ,r2):
        return r1/(r1+r2)

    out = [[name, "p >= 0.8", "p >= 0.5"],
        ["true positive rate", recall(true_positives_8, false_positives_8), recall(true_positives_5, false_positives_5)],
        ["false positive rate", recall(false_positives_8, true_positives_8), recall(false_positives_5, true_positives_5)]
        ]
    return out
return true_false("Table 1", table1) + true_false("Table2", table2)
#+end_src

#+RESULTS: bayes
| Table 1             | p >= 0.8 | p >= 0.5 |
| true positive rate  |      1.0 |      0.5 |
| false positive rate |      0.0 |      0.5 |
| Table2              | p >= 0.8 | p >= 0.5 |
| true positive rate  |      1.0 |      0.0 |
| false positive rate |      0.0 |      1.0 |

** c-d
#+begin_src jupyter-python :var data=bayes :kernel python3 :session py
import matplotlib.pyplot as plt
p1 = data[1][1], data[2][1]
p2 = data[1][2], data[2][2]
p3 = data[4][1], data[5][1]
p4 = data[4][2], data[5][2]
fig,(axes1, axes2) = plt.subplots(1, 2)

p = axes1.plot((p1[0], p1[1]),[p3[0], p3[1]])
axes1.set_title("P >= 0.8")
axes2.set_title("P >= 0.5")
p2 = axes2.plot((p2[0], p2[1]),[p4[0], p4[1]])

#+end_src

#+RESULTS:
[[file:./.ob-jupyter/6431e6f43a38bee09ec37b130315a46a024c446c.png]]
** 3.
