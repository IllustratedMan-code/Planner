% Created 2023-02-28 Tue 18:11
% Intended LaTeX compiler: lualatex
\documentclass[11pt]{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{syntax}
\usepackage{pdfpages}
\usepackage{tcolorbox}
\usepackage{etoolbox}
\usepackage[ruled]{algorithm2e}
\BeforeBeginEnvironment{minted}{\begin{tcolorbox}}%
\AfterEndEnvironment{minted}{\begin{tcolorbox}}%
\BeforeBeginEnvironment{verbatim}{\begin{tcolorbox}}%
\AfterEndEnvironment{verbatim}{\begin{tcolorbox}}%
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{physics}
\author{David Lewis}
\date{}
\title{Lecture 15}
\hypersetup{
 pdfauthor={David Lewis},
 pdftitle={Lecture 15},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 28.2 (Org mode 9.6)}, 
 pdflang={English}}
\begin{document}

\maketitle
\section*{1.}
\label{sec:org6ccb18d}
\subsection*{a.}
\label{sec:org81b1273}
\begin{itemize}
\item \(\begin{pmatrix}1 \\ 3 \\ -11 \end{pmatrix}^Tx\)
\item w = \((\vec w, -11)\)
\item \(x = (\vec x, 1)\)
\end{itemize}
\subsection*{b.}
\label{sec:orge552d2c}
\begin{itemize}
\item \(\begin{pmatrix}1 \\ 5\end{pmatrix} \rightarrow \begin{pmatrix}1 \\ 5 \\ 1\end{pmatrix}\)
\item \(\begin{pmatrix}4 \\ 1\end{pmatrix} \rightarrow \begin{pmatrix}4 \\ 1 \\ 1\end{pmatrix}\)
\end{itemize}
\subsection*{c.}
\label{sec:org9bafd5a}
\begin{itemize}
\item (1, 3, -11) * (1, 5, 1) = 1 + 15 - 11 = 5
\item (1, 3, -11) * (4, 1, 1) = 4 + 3 - 11 = -4
\end{itemize}
\subsection*{d.}
\label{sec:orgbc6e913}
\begin{itemize}
\item \(\xi_1 = 1 - 5 = -4  \rightarrow 0\)
\item \(\xi_2 = 1 - 4 = -3 \rightarrow 0\)
\end{itemize}
\section*{2.}
\label{sec:org0c4c907}

\subsection*{a.}
\label{sec:org7397a24}
\begin{itemize}
\item primal = \(\min L = \frac{1}{2}\|w\|^2 - \displaystyle \sum \limits^n_{i=1}\alpha_i(y_i(w^Tx_i +
  b)-1)\)
\item dual = \(\underset{\alpha}\max \quad L_{dual} = \displaystyle \sum\limits^n_{i=1}\alpha_i -
  \frac{1}{2}\displaystyle \sum\limits^n_{i=1}\displaystyle
  \sum\limits^n_{j=1}\alpha_i\alpha_jy_iy_jx^T_ix_j\)
\item w is the weight vector, it has d dimensions
\item \(x_i\) is a data point, it has d dimensions
\item \(y_i\) is the class label, has 1 dimension
\item \(\alpha_i, b\) are scalars
\item \(\alpha\) is the set of lagrangian multipliers, a set of scalar values that when signed
sum to zero. Additionally \(\alpha_i\) satisifies the constraints
\(\alpha_i(y_i(w^Tx_i + b)-1) = 0\) and \(\alpha_i \ge 0\)
\end{itemize}
\subsection*{b.}
\label{sec:org03bf989}
The dual can be solved using quadratic optimization. The primal is solved using
the raw constraints. The primal is easier to compute, but the dual makes the
final computation that classifies points more efficient via the kernel trick.
\section*{3.}
\label{sec:orgdbef19f}
\subsection*{a.}
\label{sec:org95f0f92}

\begin{equation}
\(L_{primal} = \frac{1}{2}\|w\|^2 + C\sum\limits^n_{i=1}\xi_i -
 \sum\limits^n_{i=1}\alpha_i(y_i(w^Tx_i + b) - 1 + \xi_i) - \sum\limits^n_{i=1}\beta_i\xi_i\)
\end{equation}

\begin{equation}
\(L_{dual} = \displaystyle \sum\limits^n_{i=1}\alpha_i -
  \frac{1}{2}\displaystyle \sum\limits^n_{i=1}\displaystyle
  \sum\limits^n_{j=1}\alpha_i\alpha_jy_iy_jx^T_ix_j\)
\end{equation}
\subsection*{b.}
\label{sec:org7f6435e}
\begin{itemize}
\item The non-linear case introduces the concept of slack \(\xi\) which indicates how
much the point violates the spearability condition (at least \(\frac{1}{\|w\|}\) away from hyperplane)
\item C is the regularization constant that controls the  the balance between
maximizing the margin and minimizing the loss (minimizing slack)
\item \(\beta\) is a second lagrangian multiplier that gets removed from the dual formation
\end{itemize}
\section*{4.}
\label{sec:orgbba6bcb}
\subsection*{a.}
\label{sec:orgc8e32c5}
\begin{center}
\begin{minipage}{0.7\linewidth}
\begin{algorithm}[H]
\caption{gradientAscent}
\SetKwFunction{SDud}{SDud}\SetKwFunction{Tol}{tol}\SetKwFunction{Break}{break}
\SetKwInOut{Input}{Input}
\Input{$x_{train}$, $y_{train}$, c, \epsilon(cutoff)}

\ForEach {$x_i \in x_{train}$}{ $x_i$ \gets \begin{pmatrix} x_i \\ 1 \end{pmatrix}  \tcp*{map to $\mathbb{R}^{d+1}$}}

\For{k = 1, ..., n}{$s_k \gets \frac{1}{x^Tx_k}$}

$t \gets 0$\;
$\alpha_0 \gets (0, ..., 0)^T$\;

\Repeat{$\|\alpha_t - \alpha_{t-1\|} \le \epsilon$}{
\alpha \gets $\alpha_t$\;
\For {k = 1 to n}{
    \tcp{update kth comoponent of $\alpha$}
    $\alpha_k \gets \alpha_k + s_k(1 - y_k)\sum^n_{i=1}\alpha_iy_ix^T_ix_k$\;
    \If{$\alpha_k < 0$}{$\alpha_k \gets 0$}
    \If{$\alpha_k > C$}{$\alpha_k \gets C$}
}

$\alpha_{t+1} = \alpha$\;
t \gets t + 1


}
\Return \alpha
\end{algorithm}
\end{minipage}
\end{center}
\subsection*{b.}
\label{sec:org9805771}
\begin{center}
\begin{minipage}{0.7\linewidth}
\begin{algorithm}[H]
\caption{learnSVM}
\SetKwFunction{SDud}{SDud}\SetKwFunction{Tol}{tol}\SetKwFunction{Break}{break}
\SetKwInOut{Input}{Input}
\Input{$x_{train}$, $y_{train}$, c, \epsilon(cutoff)}

\alpha \gets gradientAscent($x_{train}$, $y_{train}$, c, \epsilon(cutoff))\;
w \gets (0, ..., 0)^T\;
\For{$\alpha_i, y_i, x_i$ in $\alpha, y_{train}, x_{train}$}{
 $w \gets w + \alpha_iy_i(x_i, 1)$\;
}

b \gets 0\;
\For{$\alpha_i, y_i, x_i$ in $\alpha, y_{train}, x_{train}$}{
 $b \gets b + y_i - w^T(x_i, 1)$\;
}
w \gets (w, b)\;
\Return w
\end{algorithm}
\end{minipage}
\end{center}
\subsection*{c.}
\label{sec:orga6f620e}

\begin{center}
\begin{minipage}{0.7\linewidth}
\begin{algorithm}[H]
\caption{testSVM}
\SetKwFunction{SDud}{SDud}\SetKwFunction{Tol}{tol}\SetKwFunction{Break}{break}
\SetKwInOut{Input}{Input}
\Input{$x_{test}, w$}

y_{test} \gets ()^T\;

\ForEach{$x_i$ in $x_{test}$}{
$y_{test} \gets (y_{test}, w^Tx_i > 0)$
}

\Return $y_{test}$
\end{algorithm}
\end{minipage}
\end{center}
\end{document}
