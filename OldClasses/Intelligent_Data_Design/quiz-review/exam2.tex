% Created 2023-04-20 Thu 10:17
% Intended LaTeX compiler: lualatex
\documentclass[11pt]{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{syntax}
\usepackage{pdfpages}
\usepackage[most]{tcolorbox}
\usepackage{etoolbox}
\usepackage{environ}
\AtBeginEnvironment{quote}{\itshape}
\usepackage[ruled]{algorithm2e}
\let\oldtabular\tabular
\let\oldendtabular\endtabular
\NewEnviron{tabular2}[1]{\tcbox[left=0mm, right=0mm, top=0mm, bottom=0mm]{\oldtabular{#1}\BODY\oldendtabular}}
\BeforeBeginEnvironment{minted}{\begin{tcolorbox}[enhanced, breakable, skin first=enhanced, skin middle=enhanced, skin last=enhanced]}%
\AfterEndEnvironment{minted}{\end{tcolorbox}}
\BeforeBeginEnvironment{verbatim}{\begin{tcolorbox}[enhanced, breakable, skin first=enhanced, skin middle=enhanced, skin last=enhanced]}%
\AfterEndEnvironment{verbatim}{\end{tcolorbox}}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{physics}
\author{David Lewis}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={David Lewis},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 30.0.50 (Org mode 9.6.1)}, 
 pdflang={English}}
\begin{document}

\section*{formulas and tidbits}
\label{sec:orgc592848}
\begin{itemize}
\item possible clusters (brute force) =  \(k^n\)
\item Bayes: \(P(A|B) = \frac{P(A) \cdot P(B|A)}{P(B)}\)
\item Lance-williams = \(d_{(ij)k} = \alpha_id_{ik} + \alpha_jd_{jk} + \beta d_{ij} + \gamma|d_{ik} -
  d{jk}|\) d variables represent the function to determine distance between
points in each cluster
\item average distance between cluster = \[\frac{\sum_{x\inC_i}\sum_{y\in
  C_j}d(x,y)}{n_in_j}\] is the average distance between all pairs of points.
\item empirical cumulative distribution is average of all values less than or equal
to the current value in a list (can be used as step kernel).
\item Kernel density \(\hat f_k(x, h) = \frac{k(x+h/2) - k(x+h/2)}{h}\) where k is a
kernel function
\item affinity matrix = a matrix where numbers closer to one are ``similar'' used to
quantify relationships between clusters or data points
\end{itemize}
\section*{Algorithms}
\label{sec:org25bcb84}
\subsection*{K-means}
\label{sec:org799a88e}
\begin{itemize}
\item given number of clusters
\item randomly assign value to each centroid/cluster/mean
\item assign closest points in data to each cluster
\item assign centroid value with no points to random point
\item recalculate mean by taking the average of all points assigned to cluster
\item stop when means don't change
\item Time complexity: (O(n\textsuperscript{dk+1})$\backslash$)
\end{itemize}
\subsection*{EM algorithm}
\label{sec:org73c94c3}
\begin{itemize}
\item split into two steps (expectation, maximization)
\item optimize parameters that maximize the log likelyhood function
\item not really sure this lines up with what we did in class, there was some fancy
bayes, assuming c stuff going on
\item Time complexity depends on likelyhood function
\end{itemize}
\subsection*{DBSCAN}
\label{sec:org6204073}
\begin{itemize}
\item \(\epsilon\) is max distance a ``neighbor'' is allowed to be (reachable)
\item core points have more than \texttt{min\_neighbors/points}
\item border points are ``reachable'' from core points, ie there is a core point with
a border point as its neighbor
\item noise points are not reachable
\item the algorithm just finds all core points then border points, then tosses
everythings else out as noise
\end{itemize}
\subsection*{Single link/complete link}
\label{sec:org28f32ff}
\begin{itemize}
\item single link = min distance between clusters
\item complete link = max distance between clusters
\item closest clusters form new clusters where original clusters are points
\item visualized using dendrogram
\end{itemize}
\subsection*{DENCLUE}
\label{sec:org3679943}
\begin{itemize}
\item Gradient function: \[\nabla f(x) = \frac{1}{nh^{d+2}}\sum \limits^n_{i=1}K\left(\frac{x-x_i}{h}\right)(x_i-x)\]
\item Hill climbs to local min or max, then assigns points to clusters based on
local min or max
\end{itemize}
\subsection*{Spectral clustering}
\label{sec:orgd7985be}
\begin{itemize}
\item compute affinity matrix A
\item Compute Degree matrix
\begin{itemize}
\item diagonal matrix
\item each entry is the sum of weights of all edges incident with node
\end{itemize}
\item Compute L (differs with method)
\begin{itemize}
\item Average weight: L = A
\item Ratio Cut: L = D - A (used in normalized)
\item Normalized assymmetric cut: L\textsuperscript{a} = D\textsuperscript{-1} L
\item Normalized symmetric cut: L\textsuperscript{a} = D\textsuperscript{-.5} L D\textsuperscript{-.5}
\end{itemize}
\item compute k(number of clusters) smallest eigenvalues/eigenvectors of L
\item compute matrix Y from eigenvalue matrix U using \(y_i =
  \frac{1}{\sqrt{\sum^k_{j=1}u^2_{n-j+1, i}}}(\vec{u_i}^T)\)
\item find clusters using k-means on Y
\end{itemize}
\subsection*{CLIQUE}
\label{sec:orgc23d530}
\begin{itemize}
\item cuts data space into a grid of cells
\item density threshold determines if cell is ``dense''
\item adjoining dense cells are considered the same cluster
\item best for axis aligned data
\item sensitive to bin boundaries
\end{itemize}
\subsection*{Projective k-means (PCA)}
\label{sec:org5699ed8}
\begin{itemize}
\item good for data that is not axis aligned
\item PCA is hard
\end{itemize}
\subsection*{Random-projections}
\label{sec:orgf9193c2}
\begin{itemize}
\item take a random sample of points
\item test all dimensions for the following
\item A = \(\{d_k| \: \|x_p^k - x\| \le w \forall x \in s_j\}\)
\item dimension is good if there is no point that has a distance greater than w in
the particular dimensional subspace
\item good for data that is axis aligned
\end{itemize}
\subsection*{Regression}
\label{sec:org677f851}
\begin{itemize}
\item Linear regression \((D^TD)^{-1}D^Ty\)
\item Ridge regression:  \(w = (X^TX + \alpha I)^{-1}X^Ty\)
\item SSE: \(\sum(Y-\hat Y)^2\) where \(\hat Y\) is the sum of the columns after being
updated (multiplied) with the \(w\) and \(b\) found in regression
\end{itemize}
\end{document}