#+title: lec19
* 1.
a. \(4^8 \cdot 3 = 96\)
b. It takes exponential time, permutations increase exponenetially with number
   of data points
* 2.
** a.
#+NAME: assignments
#+begin_src python :session py
D = [2, 3, 4, 8, 9, 10, 15, 16, 17]
M = [5, 14, 21]
A = []
for i in D:
    L = [(i-m)**2 for m in M]
    A.append(M[L.index(min(L))])
A
#+end_src

#+RESULTS: assignments
| 5 | 5 | 5 | 5 | 5 | 14 | 14 | 14 | 14 |
** b.
#+begin_src python :session py
import random
def update_centroids(M, A):
    def reassign_empty(M, A):
        for i in M:
            if i not in A:
                new = random.choice(D)
                A[D.index(new)] = i
                return reassign_empty(M, A)
        return A
    A = reassign_empty(M, A)
    M_new = [0 for m in M]
    S_len = [0 for m in M]
    for d,a in zip(D, A):
        for m in range(len(M)):
            if a == M[m]:
                S_len[m] += 1
                M_new[m] += d
    M = [x/s for (x, s) in zip(M_new, S_len)]
    return M
M = update_centroids(M, A)
M
#+end_src

#+RESULTS:
| 5.2 | 16.0 | 10.0 |

** c.
#+NAME: assignments2
#+begin_src python :session py
D = [2, 3, 4, 8, 9, 10, 15, 16, 17]
A = []
for i in D:
    L = [(i-m)**2 for m in M]
    A.append(M[L.index(min(L))])
A
#+end_src

#+RESULTS: assignments2
| 5.2 | 5.2 | 5.2 | 10.0 | 10.0 | 10.0 | 16.0 | 16.0 | 16.0 |
** d.
In my code above, I select a random member of D to be reassigned to the empty
centroid. I made this function recursive in case the reasignment creates a new
empty centroid. The function will run until each centroid has at least one assignment.
** e.
#+begin_example
Assignment: assign each observation to the closest cluster
while assignments change:
    while centroid has no assignments:
        assign random observation to empty centroid
    recalculate centroids:
        m_i = (1/length(assignments to centroid)) * sum(each assignment to centroid)
#+end_example

Time complexity: \(O(n^{dk+1}\log n)\)

* 3.
** a.
#+name: probablility
#+begin_src python :session py2
from scipy.stats import norm
D = [1, 4, 6, 9]
Mu = [1, 9]
Sigma = [1, 1]
results = []
for mu, sigma in zip(Mu, Sigma):
    results.append([f"mu={mu}, sigma={sigma}"]+ list(norm.pdf(D, mu, sigma)))
results
#+end_src

#+RESULTS: probablility
| mu=1, sigma=1 |    0.3989422804014327 |  0.0044318484119380075 | 1.4867195147342979e-06 | 5.052271083536893e-15 |
| mu=9, sigma=1 | 5.052271083536893e-15 | 1.4867195147342979e-06 |  0.0044318484119380075 |    0.3989422804014327 |

** b.
#+begin_src python :session py2
D = list(zip(results[0][1:], results[1][1:]))
posterior = []
for i1, i2 in D:
    normalization = i1 + i2
    posterior.append([i1/(normalization), i2/(normalization)])

posterior
#+end_src

#+RESULTS:
|     0.9999999999999873 | 1.2664165549094016e-14 |
|     0.9996646498695335 |  0.0003353501304664781 |
|  0.0003353501304664781 |     0.9996646498695335 |
| 1.2664165549094016e-14 |     0.9999999999999873 |
** c.
#+begin_src python :session py2
prior = [0, 0]
for i in posterior:
    prior[0] += i[0]
    prior[1] += i[1]
prior = [i/len(posterior) for i in prior]
prior
#+end_src

#+RESULTS:
| 0.5 | 0.5 |
** d.
#+begin_src python :session py2
mu = [0, 0]
mu_d = [0, 0]
for p, (d1, d2) in zip(posterior, D):
    mu[0] += p[0] * d1
    mu[1] += p[1] * d2
    mu_d[0] += p[0]
    mu_d[1] += p[1]
mu[0] = mu[0] / mu_d[0]
mu[1] = mu[1] / mu_d[1]
mu
sigma = [0, 0]
sigma_d = [0, 0]
for p, (d1, d2) in zip(posterior, D):
    sigma[0] += p[0] * (d1 - mu[0])**2
    sigma[1] += p[1] * (d2 - mu[1])**2
    sigma_d[0] += p[0]
    sigma_d[1] += p[1]
sigma[0] = sigma[0] / sigma_d[0]
sigma[1] = sigma[1] / sigma_d[1]
sigma
[["mu_1", "mu2"], mu, ["sigma2_1", "sigma2_2"], sigma]
#+end_src

#+RESULTS:
|                mu_1 |                 mu2 |
| 0.20168632154549704 | 0.20168632154549704 |
|            sigma2_1 |            sigma2_2 |
| 0.03890991659421365 | 0.03890991659421365 |
** e
a and b are part of the E step, c and d are part of the m step
